"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[9814],{651:(r,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-03-isaac/computer-vision","title":"Chapter 3: Computer Vision with OpenCV","description":"Introduction","source":"@site/docs/module-03-isaac/03-computer-vision.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/computer-vision","permalink":"/physical-ai-textbook/docs/module-03-isaac/computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-03-isaac/03-computer-vision.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: PyBullet Basics","permalink":"/physical-ai-textbook/docs/module-03-isaac/pybullet-basics"},"next":{"title":"Chapter 4: SLAM Basics","permalink":"/physical-ai-textbook/docs/module-03-isaac/slam-basics"}}');var a=n(4848),t=n(8453);const o={sidebar_position:3},s="Chapter 3: Computer Vision with OpenCV",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Why OpenCV?",id:"why-opencv",level:3},{value:"Installation",id:"installation",level:2},{value:"Basic Installation",id:"basic-installation",level:3},{value:"Installation with ROS 2",id:"installation-with-ros-2",level:3},{value:"Basic OpenCV Usage",id:"basic-opencv-usage",level:2},{value:"Reading and Displaying Images",id:"reading-and-displaying-images",level:3},{value:"Working with Camera",id:"working-with-camera",level:3},{value:"Camera Calibration",id:"camera-calibration",level:2},{value:"Calibration Process",id:"calibration-process",level:3},{value:"Using Calibration",id:"using-calibration",level:3},{value:"Object Detection with YOLO",id:"object-detection-with-yolo",level:2},{value:"Installation",id:"installation-1",level:3},{value:"YOLO Detection with Ultralytics",id:"yolo-detection-with-ultralytics",level:3},{value:"YOLO with OpenCV DNN",id:"yolo-with-opencv-dnn",level:3},{value:"Pose Estimation",id:"pose-estimation",level:2},{value:"MediaPipe Pose Estimation (Free)",id:"mediapipe-pose-estimation-free",level:3},{value:"Face Detection",id:"face-detection",level:2},{value:"Haar Cascades (Built-in, Free)",id:"haar-cascades-built-in-free",level:3},{value:"Object Tracking",id:"object-tracking",level:2},{value:"MeanShift Tracking",id:"meanshift-tracking",level:3},{value:"CSRT Tracker",id:"csrt-tracker",level:3},{value:"ArUco Markers",id:"aruco-markers",level:2},{value:"Detection",id:"detection",level:3},{value:"Generating Markers",id:"generating-markers",level:3},{value:"Depth Estimation",id:"depth-estimation",level:2},{value:"Stereo Vision",id:"stereo-vision",level:3},{value:"Complete Perception Pipeline",id:"complete-perception-pipeline",level:2},{value:"ROS 2 Vision Node",id:"ros-2-vision-node",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Common Errors and Solutions",id:"common-errors-and-solutions",level:2},{value:"Error 1: &quot;cv2 module not found&quot;",id:"error-1-cv2-module-not-found",level:3},{value:"Error 2: &quot;Camera not opening&quot;",id:"error-2-camera-not-opening",level:3},{value:"Error 3: &quot;YOLO model not found&quot;",id:"error-3-yolo-model-not-found",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(r){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...r.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-3-computer-vision-with-opencv",children:"Chapter 3: Computer Vision with OpenCV"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"OpenCV (Open Source Computer Vision Library) is the industry-standard library for computer vision. It's completely free, open-source, and works on any computer without special hardware."}),"\n",(0,a.jsx)(e.h3,{id:"why-opencv",children:"Why OpenCV?"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Free"}),": Completely open-source"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cross-platform"}),": Windows, Linux, macOS"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Extensive"}),": 2500+ algorithms"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Well-documented"}),": Excellent resources"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Fast"}),": Optimized C++ with Python bindings"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"No GPU Required"}),": Works on CPU"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(e.h3,{id:"basic-installation",children:"Basic Installation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:'# Install OpenCV\r\npip install opencv-python\r\n\r\n# Install with contrib modules (extra features)\r\npip install opencv-contrib-python\r\n\r\n# Verify installation\r\npython3 -c "import cv2; print(cv2.__version__)"\n'})}),"\n",(0,a.jsx)(e.h3,{id:"installation-with-ros-2",children:"Installation with ROS 2"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# OpenCV is usually pre-installed with ROS 2\r\n# If not:\r\nsudo apt install python3-opencv\r\n\r\n# For C++:\r\nsudo apt install libopencv-dev\n"})}),"\n",(0,a.jsx)(e.h2,{id:"basic-opencv-usage",children:"Basic OpenCV Usage"}),"\n",(0,a.jsx)(e.h3,{id:"reading-and-displaying-images",children:"Reading and Displaying Images"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import cv2\r\nimport numpy as np\r\n\r\n# Read image\r\nimg = cv2.imread('image.jpg')\r\n\r\n# Display image\r\ncv2.imshow('Image', img)\r\ncv2.waitKey(0)\r\ncv2.destroyAllWindows()\r\n\r\n# Save image\r\ncv2.imwrite('output.jpg', img)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"working-with-camera",children:"Working with Camera"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import cv2\r\n\r\n# Open camera\r\ncap = cv2.VideoCapture(0)  # 0 for default camera\r\n\r\nwhile True:\r\n    # Read frame\r\n    ret, frame = cap.read()\r\n    \r\n    if not ret:\r\n        break\r\n    \r\n    # Process frame\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n    \r\n    # Display\r\n    cv2.imshow('Camera', frame)\r\n    \r\n    # Exit on 'q'\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# Release camera\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,a.jsx)(e.p,{children:"Camera calibration is essential for accurate measurements and 3D reconstruction."}),"\n",(0,a.jsx)(e.h3,{id:"calibration-process",children:"Calibration Process"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nCamera Calibration\r\n\r\nCalibrate camera using checkerboard pattern.\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport glob\r\n\r\n# Checkerboard dimensions (inner corners)\r\nCHECKERBOARD = (9, 6)  # 9x6 inner corners\r\n\r\n# Prepare object points\r\nobjp = np.zeros((CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\r\nobjp[:, :2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\r\n\r\n# Arrays to store object and image points\r\nobjpoints = []  # 3D points in real world\r\nimgpoints = []  # 2D points in image plane\r\n\r\n# Load calibration images\r\nimages = glob.glob(\'calibration_images/*.jpg\')\r\n\r\nfor fname in images:\r\n    img = cv2.imread(fname)\r\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n    \r\n    # Find checkerboard corners\r\n    ret, corners = cv2.findChessboardCorners(\r\n        gray,\r\n        CHECKERBOARD,\r\n        cv2.CALIB_CB_ADAPTIVE_THRESH +\r\n        cv2.CALIB_CB_FAST_CHECK +\r\n        cv2.CALIB_CB_NORMALIZE_IMAGE\r\n    )\r\n    \r\n    if ret:\r\n        objpoints.append(objp)\r\n        \r\n        # Refine corners\r\n        corners2 = cv2.cornerSubPix(\r\n            gray,\r\n            corners,\r\n            (11, 11),\r\n            (-1, -1),\r\n            (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\r\n        )\r\n        imgpoints.append(corners2)\r\n        \r\n        # Draw corners\r\n        cv2.drawChessboardCorners(img, CHECKERBOARD, corners2, ret)\r\n        cv2.imshow(\'Calibration\', img)\r\n        cv2.waitKey(500)\r\n\r\ncv2.destroyAllWindows()\r\n\r\n# Calibrate camera\r\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\r\n    objpoints,\r\n    imgpoints,\r\n    gray.shape[::-1],\r\n    None,\r\n    None\r\n)\r\n\r\n# Save calibration\r\nnp.savez(\'camera_calibration.npz\', mtx=mtx, dist=dist)\r\n\r\nprint("Camera matrix:")\r\nprint(mtx)\r\nprint("\\nDistortion coefficients:")\r\nprint(dist)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"using-calibration",children:"Using Calibration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Load calibration\r\ncalibration = np.load('camera_calibration.npz')\r\nmtx = calibration['mtx']\r\ndist = calibration['dist']\r\n\r\n# Undistort image\r\nimg = cv2.imread('test_image.jpg')\r\nundistorted = cv2.undistort(img, mtx, dist, None, mtx)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"object-detection-with-yolo",children:"Object Detection with YOLO"}),"\n",(0,a.jsx)(e.p,{children:"YOLO (You Only Look Once) is a state-of-the-art object detection system. We'll use free, pre-trained models."}),"\n",(0,a.jsx)(e.h3,{id:"installation-1",children:"Installation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Install ultralytics (YOLOv8)\r\npip install ultralytics\r\n\r\n# Or use OpenCV DNN with YOLO\r\n# Download weights from: https://github.com/AlexeyAB/darknet\n"})}),"\n",(0,a.jsx)(e.h3,{id:"yolo-detection-with-ultralytics",children:"YOLO Detection with Ultralytics"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nYOLO Object Detection\r\n\r\nDetect objects using free YOLO models.\r\n\"\"\"\r\n\r\nfrom ultralytics import YOLO\r\nimport cv2\r\n\r\n# Load pre-trained model (free, downloads automatically)\r\nmodel = YOLO('yolov8n.pt')  # nano version (fastest, smallest)\r\n\r\n# Or use other versions:\r\n# model = YOLO('yolov8s.pt')  # small\r\n# model = YOLO('yolov8m.pt')  # medium\r\n# model = YOLO('yolov8l.pt')  # large\r\n# model = YOLO('yolov8x.pt')  # extra large\r\n\r\n# Open camera\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    # Run detection\r\n    results = model(frame)\r\n    \r\n    # Draw results\r\n    annotated_frame = results[0].plot()\r\n    \r\n    # Display\r\n    cv2.imshow('YOLO Detection', annotated_frame)\r\n    \r\n    # Print detections\r\n    for result in results:\r\n        boxes = result.boxes\r\n        for box in boxes:\r\n            cls = int(box.cls[0])\r\n            conf = float(box.conf[0])\r\n            label = model.names[cls]\r\n            print(f\"Detected: {label} with confidence: {conf:.2f}\")\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"yolo-with-opencv-dnn",children:"YOLO with OpenCV DNN"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\n\r\n# Load YOLO\r\nnet = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")\r\nlayer_names = net.getLayerNames()\r\noutput_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n\r\n# Load class names\r\nwith open("coco.names", "r") as f:\r\n    classes = [line.strip() for f.readlines()]\r\n\r\n# Load image\r\nimg = cv2.imread("image.jpg")\r\nheight, width, channels = img.shape\r\n\r\n# Prepare blob\r\nblob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\r\nnet.setInput(blob)\r\nouts = net.forward(output_layers)\r\n\r\n# Process detections\r\nfor out in outs:\r\n    for detection in out:\r\n        scores = detection[5:]\r\n        class_id = np.argmax(scores)\r\n        confidence = scores[class_id]\r\n        \r\n        if confidence > 0.5:\r\n            # Get bounding box\r\n            center_x = int(detection[0] * width)\r\n            center_y = int(detection[1] * height)\r\n            w = int(detection[2] * width)\r\n            h = int(detection[3] * height)\r\n            \r\n            # Draw box\r\n            cv2.rectangle(img, (center_x - w//2, center_y - h//2),\r\n                         (center_x + w//2, center_y + h//2), (0, 255, 0), 2)\r\n            \r\n            # Draw label\r\n            label = f"{classes[class_id]}: {confidence:.2f}"\r\n            cv2.putText(img, label, (center_x, center_y),\r\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\r\n\r\ncv2.imshow("Detection", img)\r\ncv2.waitKey(0)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"pose-estimation",children:"Pose Estimation"}),"\n",(0,a.jsx)(e.h3,{id:"mediapipe-pose-estimation-free",children:"MediaPipe Pose Estimation (Free)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nPose Estimation with MediaPipe\r\n\r\nFree pose estimation for humans and objects.\r\n"""\r\n\r\nimport cv2\r\nimport mediapipe as mp\r\n\r\n# Install: pip install mediapipe\r\n\r\nmp_pose = mp.solutions.pose\r\nmp_drawing = mp.solutions.drawing_utils\r\n\r\n# Initialize pose estimation\r\npose = mp_pose.Pose(\r\n    min_detection_confidence=0.5,\r\n    min_tracking_confidence=0.5\r\n)\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    # Convert BGR to RGB\r\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n    \r\n    # Process frame\r\n    results = pose.process(rgb_frame)\r\n    \r\n    # Draw pose\r\n    if results.pose_landmarks:\r\n        mp_drawing.draw_landmarks(\r\n            frame,\r\n            results.pose_landmarks,\r\n            mp_pose.POSE_CONNECTIONS\r\n        )\r\n    \r\n    cv2.imshow(\'Pose Estimation\', frame)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n        break\r\n\r\ncap.release()\r\npose.close()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"face-detection",children:"Face Detection"}),"\n",(0,a.jsx)(e.h3,{id:"haar-cascades-built-in-free",children:"Haar Cascades (Built-in, Free)"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nFace Detection\r\n\r\nDetect faces using OpenCV's built-in Haar cascades.\r\n\"\"\"\r\n\r\nimport cv2\r\n\r\n# Load face cascade (included with OpenCV)\r\nface_cascade = cv2.CascadeClassifier(\r\n    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\r\n)\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    # Convert to grayscale\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n    \r\n    # Detect faces\r\n    faces = face_cascade.detectMultiScale(\r\n        gray,\r\n        scaleFactor=1.1,\r\n        minNeighbors=5,\r\n        minSize=(30, 30)\r\n    )\r\n    \r\n    # Draw rectangles around faces\r\n    for (x, y, w, h) in faces:\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\r\n        cv2.putText(frame, 'Face', (x, y-10),\r\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\r\n    \r\n    cv2.imshow('Face Detection', frame)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"object-tracking",children:"Object Tracking"}),"\n",(0,a.jsx)(e.h3,{id:"meanshift-tracking",children:"MeanShift Tracking"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nObject Tracking with MeanShift\r\n\r\nTrack objects in video stream.\r\n"""\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\n# Read first frame\r\nret, frame = cap.read()\r\n\r\n# Select ROI (Region of Interest)\r\nr = cv2.selectROI("Select Object", frame, False)\r\ntrack_window = r\r\n\r\n# Setup tracking\r\nhsv_roi = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\r\nmask = cv2.inRange(hsv_roi, np.array((0., 60., 32.)),\r\n                   np.array((180., 255., 255.)))\r\nroi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180])\r\ncv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\r\n\r\nterm_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\r\n    dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\r\n    \r\n    # Apply MeanShift\r\n    ret, track_window = cv2.meanShift(dst, track_window, term_crit)\r\n    \r\n    # Draw tracking box\r\n    x, y, w, h = track_window\r\n    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n    \r\n    cv2.imshow(\'Tracking\', frame)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"csrt-tracker",children:"CSRT Tracker"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import cv2\r\n\r\n# Create tracker\r\ntracker = cv2.TrackerCSRT_create()\r\n\r\ncap = cv2.VideoCapture(0)\r\nret, frame = cap.read()\r\n\r\n# Select ROI\r\nbbox = cv2.selectROI(\"Select Object\", frame, False)\r\ntracker.init(frame, bbox)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    # Update tracker\r\n    success, bbox = tracker.update(frame)\r\n    \r\n    if success:\r\n        x, y, w, h = [int(v) for v in bbox]\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n    \r\n    cv2.imshow('CSRT Tracking', frame)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"aruco-markers",children:"ArUco Markers"}),"\n",(0,a.jsx)(e.p,{children:"ArUco markers are fiducial markers used for pose estimation and camera calibration."}),"\n",(0,a.jsx)(e.h3,{id:"detection",children:"Detection"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nArUco Marker Detection\r\n\r\nDetect and estimate pose of ArUco markers.\r\n\"\"\"\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\n# Define ArUco dictionary\r\naruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_6X6_250)\r\naruco_params = cv2.aruco.DetectorParameters_create()\r\n\r\n# Camera calibration (from previous section)\r\nmtx = np.load('camera_calibration.npz')['mtx']\r\ndist = np.load('camera_calibration.npz')['dist']\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n    \r\n    # Detect markers\r\n    corners, ids, rejected = cv2.aruco.detectMarkers(\r\n        frame, aruco_dict, parameters=aruco_params\r\n    )\r\n    \r\n    if ids is not None:\r\n        # Draw detected markers\r\n        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\r\n        \r\n        # Estimate pose (if marker size known)\r\n        marker_size = 0.05  # 5cm\r\n        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(\r\n            corners, marker_size, mtx, dist\r\n        )\r\n        \r\n        # Draw axis\r\n        for i in range(len(ids)):\r\n            cv2.aruco.drawAxis(frame, mtx, dist, rvecs[i], tvecs[i], 0.03)\r\n            \r\n            # Print pose\r\n            print(f\"Marker {ids[i][0]}:\")\r\n            print(f\"  Translation: {tvecs[i][0]}\")\r\n            print(f\"  Rotation: {rvecs[i][0]}\")\r\n    \r\n    cv2.imshow('ArUco Detection', frame)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"generating-markers",children:"Generating Markers"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import cv2\r\n\r\n# Generate ArUco marker\r\naruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_6X6_250)\r\nmarker_id = 0\r\nmarker_size = 200\r\n\r\nmarker_img = cv2.aruco.drawMarker(aruco_dict, marker_id, marker_size)\r\ncv2.imwrite(f'marker_{marker_id}.png', marker_img)\r\ncv2.imshow('Marker', marker_img)\r\ncv2.waitKey(0)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"depth-estimation",children:"Depth Estimation"}),"\n",(0,a.jsx)(e.h3,{id:"stereo-vision",children:"Stereo Vision"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nStereo Depth Estimation\r\n\r\nEstimate depth using stereo cameras.\r\n\"\"\"\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\n# Stereo calibration (requires calibration of both cameras)\r\n# Load calibration\r\nstereo_calib = np.load('stereo_calibration.npz')\r\n\r\n# Create stereo matcher\r\nstereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\r\n\r\n# Open cameras (assuming two cameras)\r\ncap_left = cv2.VideoCapture(0)\r\ncap_right = cv2.VideoCapture(1)\r\n\r\nwhile True:\r\n    ret_left, frame_left = cap_left.read()\r\n    ret_right, frame_right = cap_right.read()\r\n    \r\n    if not (ret_left and ret_right):\r\n        break\r\n    \r\n    # Convert to grayscale\r\n    gray_left = cv2.cvtColor(frame_left, cv2.COLOR_BGR2GRAY)\r\n    gray_right = cv2.cvtColor(frame_right, cv2.COLOR_BGR2GRAY)\r\n    \r\n    # Compute disparity\r\n    disparity = stereo.compute(gray_left, gray_right)\r\n    \r\n    # Normalize for display\r\n    disparity_normalized = cv2.normalize(\r\n        disparity, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U\r\n    )\r\n    \r\n    # Apply colormap\r\n    disparity_colored = cv2.applyColorMap(disparity_normalized, cv2.COLORMAP_JET)\r\n    \r\n    cv2.imshow('Left', frame_left)\r\n    cv2.imshow('Right', frame_right)\r\n    cv2.imshow('Depth', disparity_colored)\r\n    \r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap_left.release()\r\ncap_right.release()\r\ncv2.destroyAllWindows()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"complete-perception-pipeline",children:"Complete Perception Pipeline"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-vision-node",children:"ROS 2 Vision Node"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete Vision Pipeline\r\n\r\nCombines detection, tracking, and pose estimation.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport cv2\r\n\r\n\r\nclass VisionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vision_pipeline\')\r\n        \r\n        # Initialize components\r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        \r\n        # Subscriber\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(Image, \'/vision/detections\', 10)\r\n        \r\n        self.get_logger().info(\'Vision pipeline started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Process incoming image."""\r\n        try:\r\n            # Convert ROS image to OpenCV\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Run YOLO detection\r\n            results = self.yolo_model(cv_image)\r\n            \r\n            # Draw results\r\n            annotated = results[0].plot()\r\n            \r\n            # Convert back to ROS\r\n            ros_image = self.bridge.cv2_to_imgmsg(annotated, "bgr8")\r\n            ros_image.header = msg.header\r\n            \r\n            # Publish\r\n            self.detection_pub.publish(ros_image)\r\n            \r\n            # Log detections\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    cls = int(box.cls[0])\r\n                    conf = float(box.conf[0])\r\n                    label = self.yolo_model.names[cls]\r\n                    self.get_logger().info(\r\n                        f"Detected: {label} ({conf:.2f})"\r\n                    )\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error processing image: {e}\')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionPipeline()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Use appropriate models"}),": Choose YOLO size based on speed/accuracy needs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Optimize for real-time"}),": Reduce image resolution if needed"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cache models"}),": Load models once, reuse"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Error handling"}),": Always handle camera/image errors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Resource management"}),": Release cameras and windows properly"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"common-errors-and-solutions",children:"Common Errors and Solutions"}),"\n",(0,a.jsx)(e.h3,{id:"error-1-cv2-module-not-found",children:'Error 1: "cv2 module not found"'}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Solution\r\npip install opencv-python\n"})}),"\n",(0,a.jsx)(e.h3,{id:"error-2-camera-not-opening",children:'Error 2: "Camera not opening"'}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Solution: Check camera index\r\nfor i in range(10):\r\n    cap = cv2.VideoCapture(i)\r\n    if cap.isOpened():\r\n        print(f"Camera found at index {i}")\r\n        break\n'})}),"\n",(0,a.jsx)(e.h3,{id:"error-3-yolo-model-not-found",children:'Error 3: "YOLO model not found"'}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Solution: Model downloads automatically on first use\r\n# Or download manually from ultralytics website\n"})}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"Continue learning:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"/physical-ai-textbook/docs/module-03-isaac/slam-basics",children:"Chapter 4: SLAM Basics"})," - Build maps for free"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"/physical-ai-textbook/docs/module-03-isaac/navigation",children:"Chapter 5: Navigation"})," - Navigate autonomously"]}),"\n"]})]})}function m(r={}){const{wrapper:e}={...(0,t.R)(),...r.components};return e?(0,a.jsx)(e,{...r,children:(0,a.jsx)(d,{...r})}):d(r)}},8453:(r,e,n)=>{n.d(e,{R:()=>o,x:()=>s});var i=n(6540);const a={},t=i.createContext(a);function o(r){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof r?r(e):{...e,...r}},[e,r])}function s(r){let e;return e=r.disableParentContext?"function"==typeof r.components?r.components(a):r.components||a:o(r.components),i.createElement(t.Provider,{value:e},r.children)}}}]);