"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[4555],{8:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-04-vla/chapter-02-multimodal-learning","title":"Chapter 2: Multimodal Learning","description":"Introduction","source":"@site/docs/module-04-vla/chapter-02-multimodal-learning.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/chapter-02-multimodal-learning","permalink":"/physical-ai-textbook/docs/module-04-vla/chapter-02-multimodal-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/chapter-02-multimodal-learning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}}');var t=r(4848),s=r(8453);const a={sidebar_position:3},l="Chapter 2: Multimodal Learning",o={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Multimodal Representations",id:"multimodal-representations",level:2},{value:"Early Fusion",id:"early-fusion",level:3},{value:"Late Fusion",id:"late-fusion",level:3},{value:"Cross-Modal Attention",id:"cross-modal-attention",level:2},{value:"Attention Mechanism",id:"attention-mechanism",level:3},{value:"CLIP-style Learning",id:"clip-style-learning",level:2},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Transformer-based Fusion",id:"transformer-based-fusion",level:2},{value:"Multimodal Transformer",id:"multimodal-transformer",level:3},{value:"Perceiver Architecture",id:"perceiver-architecture",level:2},{value:"Cross-Attention Perceiver",id:"cross-attention-perceiver",level:3},{value:"Training Strategies",id:"training-strategies",level:2},{value:"Pre-training",id:"pre-training",level:3},{value:"Multi-task Learning",id:"multi-task-learning",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-2-multimodal-learning",children:"Chapter 2: Multimodal Learning"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Multimodal learning is the core of VLA systems, enabling models to understand and reason about information from multiple modalities simultaneously."}),"\n",(0,t.jsx)(n.h2,{id:"multimodal-representations",children:"Multimodal Representations"}),"\n",(0,t.jsx)(n.h3,{id:"early-fusion",children:"Early Fusion"}),"\n",(0,t.jsx)(n.p,{children:"Combine modalities at input level:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass EarlyFusion(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vision_encoder = VisionEncoder()\r\n        self.language_encoder = LanguageEncoder()\r\n        self.fusion = nn.Linear(vision_dim + lang_dim, hidden_dim)\r\n    \r\n    def forward(self, image, text):\r\n        v_features = self.vision_encoder(image)\r\n        l_features = self.language_encoder(text)\r\n        combined = torch.cat([v_features, l_features], dim=-1)\r\n        fused = self.fusion(combined)\r\n        return fused\n"})}),"\n",(0,t.jsx)(n.h3,{id:"late-fusion",children:"Late Fusion"}),"\n",(0,t.jsx)(n.p,{children:"Fuse after encoding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class LateFusion(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vision_encoder = VisionEncoder()\r\n        self.language_encoder = LanguageEncoder()\r\n        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)\r\n    \r\n    def forward(self, image, text):\r\n        v_features = self.vision_encoder(image)\r\n        l_features = self.language_encoder(text)\r\n        \r\n        # Cross-attention\r\n        fused, _ = self.cross_attention(\r\n            v_features, l_features, l_features\r\n        )\r\n        return fused\n"})}),"\n",(0,t.jsx)(n.h2,{id:"cross-modal-attention",children:"Cross-Modal Attention"}),"\n",(0,t.jsx)(n.h3,{id:"attention-mechanism",children:"Attention Mechanism"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class CrossModalAttention(nn.Module):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.query = nn.Linear(dim, dim)\r\n        self.key = nn.Linear(dim, dim)\r\n        self.value = nn.Linear(dim, dim)\r\n    \r\n    def forward(self, vision, language):\r\n        q = self.query(vision)\r\n        k = self.key(language)\r\n        v = self.value(language)\r\n        \r\n        scores = torch.matmul(q, k.transpose(-2, -1))\r\n        attention = torch.softmax(scores, dim=-1)\r\n        output = torch.matmul(attention, v)\r\n        return output\n"})}),"\n",(0,t.jsx)(n.h2,{id:"clip-style-learning",children:"CLIP-style Learning"}),"\n",(0,t.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch.nn.functional as F\r\n\r\ndef clip_loss(vision_features, language_features, temperature=0.07):\r\n    # Normalize\r\n    vision_features = F.normalize(vision_features, dim=-1)\r\n    language_features = F.normalize(language_features, dim=-1)\r\n    \r\n    # Compute similarity\r\n    logits = torch.matmul(vision_features, language_features.T) / temperature\r\n    \r\n    # Contrastive loss\r\n    labels = torch.arange(len(vision_features))\r\n    loss = F.cross_entropy(logits, labels)\r\n    return loss\n"})}),"\n",(0,t.jsx)(n.h2,{id:"transformer-based-fusion",children:"Transformer-based Fusion"}),"\n",(0,t.jsx)(n.h3,{id:"multimodal-transformer",children:"Multimodal Transformer"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from transformers import BertModel, ViTModel\r\n\r\nclass MultimodalTransformer(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vision_encoder = ViTModel.from_pretrained('google/vit-base')\r\n        self.language_encoder = BertModel.from_pretrained('bert-base')\r\n        \r\n        # Fusion transformer\r\n        self.fusion_layers = nn.TransformerEncoder(\r\n            nn.TransformerEncoderLayer(d_model=768, nhead=12),\r\n            num_layers=6\r\n        )\r\n    \r\n    def forward(self, image, text):\r\n        v_features = self.vision_encoder(image).last_hidden_state\r\n        l_features = self.language_encoder(text).last_hidden_state\r\n        \r\n        # Concatenate and fuse\r\n        combined = torch.cat([v_features, l_features], dim=1)\r\n        fused = self.fusion_layers(combined)\r\n        return fused\n"})}),"\n",(0,t.jsx)(n.h2,{id:"perceiver-architecture",children:"Perceiver Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"cross-attention-perceiver",children:"Cross-Attention Perceiver"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PerceiverVL(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.latent = nn.Parameter(torch.randn(256, 768))\r\n        self.cross_attn = nn.MultiheadAttention(768, num_heads=12)\r\n        self.self_attn = nn.TransformerEncoderLayer(768, nhead=12)\r\n    \r\n    def forward(self, vision, language):\r\n        # Cross-attend to inputs\r\n        latent, _ = self.cross_attn(\r\n            self.latent,\r\n            torch.cat([vision, language], dim=1),\r\n            torch.cat([vision, language], dim=1)\r\n        )\r\n        \r\n        # Self-attention\r\n        output = self.self_attn(latent)\r\n        return output\n"})}),"\n",(0,t.jsx)(n.h2,{id:"training-strategies",children:"Training Strategies"}),"\n",(0,t.jsx)(n.h3,{id:"pre-training",children:"Pre-training"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language Pre-training"}),": Learn general representations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task-specific Fine-tuning"}),": Adapt to robotics"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"multi-task-learning",children:"Multi-task Learning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class MultiTaskVLA(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.shared_encoder = MultimodalEncoder()\r\n        self.action_head = ActionHead()\r\n        self.vqa_head = VQAHead()\r\n    \r\n    def forward(self, image, text, task='action'):\r\n        features = self.shared_encoder(image, text)\r\n        \r\n        if task == 'action':\r\n            return self.action_head(features)\r\n        elif task == 'vqa':\r\n            return self.vqa_head(features)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use pre-trained encoders when possible"}),"\n",(0,t.jsx)(n.li,{children:"Align feature spaces across modalities"}),"\n",(0,t.jsx)(n.li,{children:"Use appropriate fusion strategies"}),"\n",(0,t.jsx)(n.li,{children:"Regularize to prevent overfitting"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement early fusion model"}),"\n",(0,t.jsx)(n.li,{children:"Add cross-modal attention"}),"\n",(0,t.jsx)(n.li,{children:"Train on multimodal dataset"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Chapter 3 covers language grounding in visual scenes."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>l});var i=r(6540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);