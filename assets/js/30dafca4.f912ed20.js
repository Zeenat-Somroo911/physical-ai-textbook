"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[3873],{7366:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"projects/project-03-vla-integration","title":"Project 3: VLA Integration","description":"Objective","source":"@site/docs/projects/project-03-vla-integration.md","sourceDirName":"projects","slug":"/projects/project-03-vla-integration","permalink":"/physical-ai-textbook/docs/projects/project-03-vla-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/projects/project-03-vla-integration.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4}}');var t=r(4848),s=r(8453);const o={sidebar_position:4},l="Project 3: VLA Integration",a={},c=[{value:"Objective",id:"objective",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"Step 1: VLA Model Wrapper",id:"step-1-vla-model-wrapper",level:2},{value:"Step 2: Integration with Simulation",id:"step-2-integration-with-simulation",level:2},{value:"Step 3: Testing",id:"step-3-testing",level:2},{value:"Step 4: Advanced VLA Integration",id:"step-4-advanced-vla-integration",level:2},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Testing Checklist",id:"testing-checklist",level:2},{value:"Extensions",id:"extensions",level:2},{value:"Conclusion",id:"conclusion",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"project-3-vla-integration",children:"Project 3: VLA Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"objective",children:"Objective"}),"\n",(0,t.jsx)(n.p,{children:"Integrate a Vision-Language-Action model with a ROS2 robot system to enable natural language control."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of all modules"}),"\n",(0,t.jsx)(n.li,{children:"PyTorch installed"}),"\n",(0,t.jsx)(n.li,{children:"Pre-trained VLA model (or ability to train one)"}),"\n",(0,t.jsx)(n.li,{children:"ROS2 setup"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.p,{children:"This project creates a ROS2 node that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Receives camera images"}),"\n",(0,t.jsx)(n.li,{children:"Processes natural language instructions"}),"\n",(0,t.jsx)(n.li,{children:"Uses VLA model to predict actions"}),"\n",(0,t.jsx)(n.li,{children:"Publishes robot commands"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"step-1-vla-model-wrapper",children:"Step 1: VLA Model Wrapper"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"vla_integration/nodes/vla_node.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nimport torchvision.transforms as transforms\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\r\nclass VLARobotNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vla_robot_node')\r\n        \r\n        # Initialize VLA model\r\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n        self.load_vla_model()\r\n        \r\n        # Image processing\r\n        self.bridge = CvBridge()\r\n        self.transform = transforms.Compose([\r\n            transforms.ToPILImage(),\r\n            transforms.Resize((224, 224)),\r\n            transforms.ToTensor(),\r\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                               std=[0.229, 0.224, 0.225])\r\n        ])\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.instruction_sub = self.create_subscription(\r\n            String,\r\n            '/instruction',\r\n            self.instruction_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher\r\n        self.cmd_pub = self.create_publisher(\r\n            Twist,\r\n            '/cmd_vel',\r\n            10\r\n        )\r\n        \r\n        self.current_image = None\r\n        self.current_instruction = None\r\n        \r\n        self.get_logger().info('VLA Robot Node started')\r\n    \r\n    def load_vla_model(self):\r\n        \"\"\"Load pre-trained VLA model\"\"\"\r\n        # This is a placeholder - replace with your actual model\r\n        self.get_logger().info('Loading VLA model...')\r\n        # self.model = YourVLAModel()\r\n        # self.model.eval()\r\n        self.get_logger().info('VLA model loaded')\r\n    \r\n    def image_callback(self, msg):\r\n        \"\"\"Store latest image\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"rgb8\")\r\n            self.current_image = cv_image\r\n            self.process_if_ready()\r\n        except Exception as e:\r\n            self.get_logger().error(f'Error processing image: {e}')\r\n    \r\n    def instruction_callback(self, msg):\r\n        \"\"\"Store latest instruction\"\"\"\r\n        self.current_instruction = msg.data\r\n        self.get_logger().info(f'Received instruction: {self.current_instruction}')\r\n        self.process_if_ready()\r\n    \r\n    def process_if_ready(self):\r\n        \"\"\"Process when both image and instruction are available\"\"\"\r\n        if self.current_image is not None and self.current_instruction is not None:\r\n            action = self.predict_action(self.current_image, self.current_instruction)\r\n            self.publish_action(action)\r\n    \r\n    def predict_action(self, image, instruction):\r\n        \"\"\"Use VLA model to predict action\"\"\"\r\n        # Preprocess image\r\n        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\r\n        \r\n        # Tokenize instruction\r\n        # tokens = self.tokenizer(instruction, return_tensors='pt').to(self.device)\r\n        \r\n        # Predict action (placeholder)\r\n        # with torch.no_grad():\r\n        #     action = self.model(image_tensor, tokens)\r\n        \r\n        # For now, return a simple action based on keywords\r\n        action = self.simple_action_parser(instruction)\r\n        return action\r\n    \r\n    def simple_action_parser(self, instruction):\r\n        \"\"\"Simple keyword-based action parser (replace with actual VLA model)\"\"\"\r\n        instruction_lower = instruction.lower()\r\n        \r\n        # Parse movement commands\r\n        linear_x = 0.0\r\n        angular_z = 0.0\r\n        \r\n        if 'forward' in instruction_lower or 'ahead' in instruction_lower:\r\n            linear_x = 0.5\r\n        elif 'backward' in instruction_lower or 'back' in instruction_lower:\r\n            linear_x = -0.5\r\n        elif 'stop' in instruction_lower or 'halt' in instruction_lower:\r\n            linear_x = 0.0\r\n            angular_z = 0.0\r\n        \r\n        if 'left' in instruction_lower:\r\n            angular_z = 0.5\r\n        elif 'right' in instruction_lower:\r\n            angular_z = -0.5\r\n        \r\n        return {'linear_x': linear_x, 'angular_z': angular_z}\r\n    \r\n    def publish_action(self, action):\r\n        \"\"\"Publish robot command\"\"\"\r\n        cmd = Twist()\r\n        cmd.linear.x = action.get('linear_x', 0.0)\r\n        cmd.angular.z = action.get('angular_z', 0.0)\r\n        \r\n        self.cmd_pub.publish(cmd)\r\n        self.get_logger().info(f'Publishing command: linear={cmd.linear.x}, angular={cmd.angular.z}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VLARobotNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-2-integration-with-simulation",children:"Step 2: Integration with Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Update launch file to include VLA node:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # ... existing simulation launch ...\r\n        \r\n        # VLA Node\r\n        Node(\r\n            package='vla_integration',\r\n            executable='vla_node',\r\n            name='vla_robot_node',\r\n            output='screen'\r\n        )\r\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-3-testing",children:"Step 3: Testing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch simulation with VLA node\r\nros2 launch vla_integration complete_system.launch.py\r\n\r\n# Send instruction\r\nros2 topic pub /instruction std_msgs/msg/String \"data: 'move forward'\"\r\n\r\n# Monitor commands\r\nros2 topic echo /cmd_vel\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-4-advanced-vla-integration",children:"Step 4: Advanced VLA Integration"}),"\n",(0,t.jsx)(n.p,{children:"For a real VLA model, you would:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load Pre-trained Model"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from transformers import AutoModel, AutoTokenizer\r\n\r\nmodel = AutoModel.from_pretrained('your-vla-model')\r\ntokenizer = AutoTokenizer.from_pretrained('your-vla-model')\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Process Multimodal Input"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Encode image and text\r\nimage_features = vision_encoder(image)\r\ntext_features = language_encoder(instruction)\r\nfused_features = multimodal_fusion(image_features, text_features)\r\naction = action_decoder(fused_features)\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle Action Space"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Convert model output to robot commands\r\ndef action_to_twist(action_tensor):\r\n    # Map model output to Twist message\r\n    twist = Twist()\r\n    twist.linear.x = action_tensor[0].item()\r\n    twist.angular.z = action_tensor[1].item()\r\n    return twist\n"})}),"\n",(0,t.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SafeVLANode(VLARobotNode):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.max_velocity = 1.0\r\n        self.max_angular = 1.0\r\n    \r\n    def publish_action(self, action):\r\n        # Constrain actions for safety\r\n        linear_x = np.clip(action['linear_x'], -self.max_velocity, self.max_velocity)\r\n        angular_z = np.clip(action['angular_z'], -self.max_angular, self.max_angular)\r\n        \r\n        cmd = Twist()\r\n        cmd.linear.x = linear_x\r\n        cmd.angular.z = angular_z\r\n        self.cmd_pub.publish(cmd)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-checklist",children:"Testing Checklist"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","VLA node receives images"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Instructions are processed correctly"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Actions are predicted"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Commands are published"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Robot responds appropriately"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Safety constraints are enforced"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate with real VLA model (RT-1, RT-2, etc.)"}),"\n",(0,t.jsx)(n.li,{children:"Add action history for temporal reasoning"}),"\n",(0,t.jsx)(n.li,{children:"Implement feedback mechanism"}),"\n",(0,t.jsx)(n.li,{children:"Add confidence scoring"}),"\n",(0,t.jsx)(n.li,{children:"Create web interface for instructions"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"You've successfully integrated a VLA system with ROS2! This enables natural language control of robots, opening up many possibilities for human-robot interaction."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Experiment with different VLA architectures"}),"\n",(0,t.jsx)(n.li,{children:"Collect real-world data for fine-tuning"}),"\n",(0,t.jsx)(n.li,{children:"Deploy on physical robot"}),"\n",(0,t.jsx)(n.li,{children:"Explore multi-modal instructions (voice + text)"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);