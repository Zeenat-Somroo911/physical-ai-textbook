"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[6580],{8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>t});var r=i(6540);const s={},a=r.createContext(s);function l(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),r.createElement(a.Provider,{value:e},n.children)}},9126:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>o,contentTitle:()=>t,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-04-vla/chapter-01-vla-foundations","title":"Chapter 1: VLA Foundations","description":"Introduction","source":"@site/docs/module-04-vla/chapter-01-vla-foundations.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/chapter-01-vla-foundations","permalink":"/physical-ai-textbook/docs/module-04-vla/chapter-01-vla-foundations","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/chapter-01-vla-foundations.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2}}');var s=i(4848),a=i(8453);const l={sidebar_position:2},t="Chapter 1: VLA Foundations",o={},d=[{value:"Introduction",id:"introduction",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Multimodal Understanding",id:"multimodal-understanding",level:3},{value:"Embodied AI",id:"embodied-ai",level:3},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Typical VLA Pipeline",id:"typical-vla-pipeline",level:3},{value:"Vision Encoders",id:"vision-encoders",level:2},{value:"Convolutional Neural Networks",id:"convolutional-neural-networks",level:3},{value:"Vision Transformers",id:"vision-transformers",level:3},{value:"Language Encoders",id:"language-encoders",level:2},{value:"Transformer-based",id:"transformer-based",level:3},{value:"Action Decoders",id:"action-decoders",level:2},{value:"Continuous Actions",id:"continuous-actions",level:3},{value:"Training Paradigms",id:"training-paradigms",level:2},{value:"Supervised Learning",id:"supervised-learning",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:3},{value:"Imitation Learning",id:"imitation-learning",level:3},{value:"Datasets",id:"datasets",level:2},{value:"Popular VLA Datasets",id:"popular-vla-datasets",level:3},{value:"Challenges",id:"challenges",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-1-vla-foundations",children:"Chapter 1: VLA Foundations"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"Vision-Language-Action (VLA) models represent a paradigm shift in robotics, enabling robots to understand natural language instructions and execute tasks by combining visual perception with language understanding."}),"\n",(0,s.jsx)(e.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,s.jsx)(e.p,{children:"VLA models integrate three key components:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision"}),": Understanding visual scenes"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language"}),": Processing natural language instructions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action"}),": Generating appropriate robot actions"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsx)(e.h3,{id:"multimodal-understanding",children:"Multimodal Understanding"}),"\n",(0,s.jsx)(e.p,{children:"VLA models learn joint representations of:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Visual observations (images, point clouds)"}),"\n",(0,s.jsx)(e.li,{children:"Language instructions (text, speech)"}),"\n",(0,s.jsx)(e.li,{children:"Action sequences (motor commands)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"embodied-ai",children:"Embodied AI"}),"\n",(0,s.jsx)(e.p,{children:"Unlike traditional AI, VLA operates in:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Physical environments"}),"\n",(0,s.jsx)(e.li,{children:"Real-time constraints"}),"\n",(0,s.jsx)(e.li,{children:"Unstructured settings"}),"\n",(0,s.jsx)(e.li,{children:"Human-robot interaction"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(e.h3,{id:"typical-vla-pipeline",children:"Typical VLA Pipeline"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Input: Image + Language Instruction\r\n  \u2193\r\nVision Encoder (CNN/ViT)\r\n  \u2193\r\nLanguage Encoder (Transformer)\r\n  \u2193\r\nMultimodal Fusion\r\n  \u2193\r\nAction Decoder\r\n  \u2193\r\nOutput: Robot Actions\n"})}),"\n",(0,s.jsx)(e.h2,{id:"vision-encoders",children:"Vision Encoders"}),"\n",(0,s.jsx)(e.h3,{id:"convolutional-neural-networks",children:"Convolutional Neural Networks"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass VisionEncoder(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.backbone = nn.Sequential(\r\n            nn.Conv2d(3, 64, 3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(2),\r\n            # ... more layers\r\n        )\r\n    \r\n    def forward(self, image):\r\n        features = self.backbone(image)\r\n        return features\n"})}),"\n",(0,s.jsx)(e.h3,{id:"vision-transformers",children:"Vision Transformers"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from transformers import ViTModel\r\n\r\nvision_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n"})}),"\n",(0,s.jsx)(e.h2,{id:"language-encoders",children:"Language Encoders"}),"\n",(0,s.jsx)(e.h3,{id:"transformer-based",children:"Transformer-based"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from transformers import BertModel\r\n\r\nlanguage_encoder = BertModel.from_pretrained('bert-base-uncased')\n"})}),"\n",(0,s.jsx)(e.h2,{id:"action-decoders",children:"Action Decoders"}),"\n",(0,s.jsx)(e.h3,{id:"continuous-actions",children:"Continuous Actions"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ActionDecoder(nn.Module):\r\n    def __init__(self, hidden_dim, action_dim):\r\n        super().__init__()\r\n        self.fc = nn.Sequential(\r\n            nn.Linear(hidden_dim, 256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, action_dim)\r\n        )\r\n    \r\n    def forward(self, features):\r\n        actions = self.fc(features)\r\n        return actions\n"})}),"\n",(0,s.jsx)(e.h2,{id:"training-paradigms",children:"Training Paradigms"}),"\n",(0,s.jsx)(e.h3,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Learn from demonstration"}),"\n",(0,s.jsx)(e.li,{children:"Paired (image, language, action) data"}),"\n",(0,s.jsx)(e.li,{children:"Direct supervision"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Learn from trial and error"}),"\n",(0,s.jsx)(e.li,{children:"Reward-based learning"}),"\n",(0,s.jsx)(e.li,{children:"Exploration strategies"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Learn from expert demonstrations"}),"\n",(0,s.jsx)(e.li,{children:"Behavioral cloning"}),"\n",(0,s.jsx)(e.li,{children:"Dataset aggregation (DAgger)"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"datasets",children:"Datasets"}),"\n",(0,s.jsx)(e.h3,{id:"popular-vla-datasets",children:"Popular VLA Datasets"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ALFRED"}),": Household tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"CALVIN"}),": Manipulation tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"SayCan"}),": Language-conditioned robotics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RT-1/RT-2"}),": Real-world robot data"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"challenges",children:"Challenges"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sim-to-real gap"}),": Transfer from simulation to reality"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Generalization"}),": Handling unseen scenarios"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Temporal reasoning"}),": Understanding sequences"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety"}),": Ensuring safe actions"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"Chapter 2 explores multimodal learning in detail."})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}}}]);