"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[8796],{8399:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-03-isaac/perception-pipeline","title":"Chapter 6: Complete Perception Pipeline","description":"Introduction","source":"@site/docs/module-03-isaac/06-perception-pipeline.md","sourceDirName":"module-03-isaac","slug":"/module-03-isaac/perception-pipeline","permalink":"/physical-ai-textbook/docs/module-03-isaac/perception-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-03-isaac/06-perception-pipeline.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Navigation with Nav2","permalink":"/physical-ai-textbook/docs/module-03-isaac/navigation"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/physical-ai-textbook/docs/module-04-vla/intro"}}');var o=r(4848),i=r(8453);const t={sidebar_position:6},l="Chapter 6: Complete Perception Pipeline",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Complete Autonomous System",id:"complete-autonomous-system",level:2},{value:"System Components",id:"system-components",level:3},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:2},{value:"Combining Multiple Sensors",id:"combining-multiple-sensors",level:3},{value:"Object Detection + SLAM + Nav2 Integration",id:"object-detection--slam--nav2-integration",level:2},{value:"Complete System",id:"complete-system",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Deployment Tips",id:"deployment-tips",level:2},{value:"1. Resource Management",id:"1-resource-management",level:3},{value:"2. Error Recovery",id:"2-error-recovery",level:3},{value:"3. Logging and Monitoring",id:"3-logging-and-monitoring",level:3},{value:"4. Configuration Files",id:"4-configuration-files",level:3},{value:"Complete Launch File",id:"complete-launch-file",level:2},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Test Checklist",id:"test-checklist",level:3},{value:"Module 3 Summary",id:"module-3-summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-6-complete-perception-pipeline",children:"Chapter 6: Complete Perception Pipeline"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"This chapter integrates all the free tools we've learned into a complete autonomous perception and navigation system. We'll combine object detection, SLAM, and navigation to create a fully functional autonomous robot."}),"\n",(0,o.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TB\r\n    A[Sensors] --\x3e B[Vision Pipeline]\r\n    A --\x3e C[SLAM System]\r\n    A --\x3e D[Navigation Stack]\r\n    \r\n    B --\x3e E[Object Detection]\r\n    B --\x3e F[Tracking]\r\n    B --\x3e G[Pose Estimation]\r\n    \r\n    C --\x3e H[Map Building]\r\n    C --\x3e I[Localization]\r\n    \r\n    D --\x3e J[Path Planning]\r\n    D --\x3e K[Obstacle Avoidance]\r\n    \r\n    E --\x3e L[Decision Making]\r\n    F --\x3e L\r\n    G --\x3e L\r\n    H --\x3e L\r\n    I --\x3e L\r\n    J --\x3e L\r\n    K --\x3e L\r\n    \r\n    L --\x3e M[Robot Control]\r\n    \r\n    style A fill:#e1f5ff\r\n    style L fill:#c8e6c9\r\n    style M fill:#fff9c4\n"})}),"\n",(0,o.jsx)(n.h2,{id:"complete-autonomous-system",children:"Complete Autonomous System"}),"\n",(0,o.jsx)(n.h3,{id:"system-components",children:"System Components"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision Pipeline"}),": OpenCV + YOLO for object detection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"SLAM System"}),": ORB-SLAM3 or RTAB-Map for mapping"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Stack"}),": Nav2 for path planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Fusion"}),": Combine multiple sensor inputs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision Making"}),": High-level task planning"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete Perception Pipeline\r\n\r\nIntegrates vision, SLAM, and navigation.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu\r\nfrom nav_msgs.msg import OccupancyGrid, Odometry\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass PerceptionPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__(\'perception_pipeline\')\r\n        \r\n        # Initialize components\r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        \r\n        # State\r\n        self.current_map = None\r\n        self.current_pose = None\r\n        self.detected_objects = []\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10\r\n        )\r\n        self.map_sub = self.create_subscription(\r\n            OccupancyGrid, \'/map\', self.map_callback, 10\r\n        )\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry, \'/odom\', self.odom_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.detection_pub = self.create_publisher(Image, \'/detections\', 10)\r\n        \r\n        # Timer for control loop\r\n        self.timer = self.create_timer(0.1, self.control_loop)\r\n        \r\n        self.get_logger().info(\'Perception pipeline started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Process camera images."""\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Run object detection\r\n            results = self.yolo_model(cv_image)\r\n            \r\n            # Store detections\r\n            self.detected_objects = []\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    cls = int(box.cls[0])\r\n                    conf = float(box.conf[0])\r\n                    label = self.yolo_model.names[cls]\r\n                    \r\n                    if conf > 0.5:  # Confidence threshold\r\n                        self.detected_objects.append({\r\n                            \'label\': label,\r\n                            \'confidence\': conf,\r\n                            \'bbox\': box.xyxy[0].cpu().numpy()\r\n                        })\r\n            \r\n            # Draw and publish\r\n            annotated = results[0].plot()\r\n            ros_image = self.bridge.cv2_to_imgmsg(annotated, "bgr8")\r\n            ros_image.header = msg.header\r\n            self.detection_pub.publish(ros_image)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Image processing error: {e}\')\r\n    \r\n    def scan_callback(self, msg):\r\n        """Process LiDAR scans."""\r\n        # Store scan data for obstacle avoidance\r\n        self.latest_scan = msg\r\n    \r\n    def map_callback(self, msg):\r\n        """Process map updates."""\r\n        self.current_map = msg\r\n    \r\n    def odom_callback(self, msg):\r\n        """Process odometry."""\r\n        self.current_pose = msg.pose.pose\r\n    \r\n    def control_loop(self):\r\n        """Main control loop."""\r\n        if self.current_pose is None:\r\n            return\r\n        \r\n        cmd = Twist()\r\n        \r\n        # Simple obstacle avoidance\r\n        if self.latest_scan:\r\n            # Find minimum distance\r\n            ranges = [r for r in self.latest_scan.ranges \r\n                     if r > self.latest_scan.range_min \r\n                     and r < self.latest_scan.range_max]\r\n            \r\n            if ranges:\r\n                min_dist = min(ranges)\r\n                \r\n                if min_dist < 0.5:  # Too close\r\n                    cmd.angular.z = 0.5  # Turn\r\n                else:\r\n                    cmd.linear.x = 0.2  # Move forward\r\n        \r\n        self.cmd_vel_pub.publish(cmd)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = PerceptionPipeline()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,o.jsx)(n.h3,{id:"combining-multiple-sensors",children:"Combining Multiple Sensors"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nSensor Fusion\r\n\r\nCombines camera, LiDAR, and IMU data.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, LaserScan, Imu\r\nfrom geometry_msgs.msg import Twist\r\nimport numpy as np\r\nfrom scipy.spatial.transform import Rotation\r\n\r\nclass SensorFusion(Node):\r\n    def __init__(self):\r\n        super().__init__(\'sensor_fusion\')\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, \'/scan\', self.scan_callback, 10\r\n        )\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, \'/imu/data\', self.imu_callback, 10\r\n        )\r\n        \r\n        # State\r\n        self.image_data = None\r\n        self.scan_data = None\r\n        self.imu_data = None\r\n        \r\n        # Timer for fusion\r\n        self.timer = self.create_timer(0.1, self.fuse_sensors)\r\n        \r\n        self.get_logger().info(\'Sensor fusion started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Store image data."""\r\n        self.image_data = msg\r\n    \r\n    def scan_callback(self, msg):\r\n        """Store scan data."""\r\n        self.scan_data = msg\r\n    \r\n    def imu_callback(self, msg):\r\n        """Store IMU data."""\r\n        self.imu_data = msg\r\n    \r\n    def fuse_sensors(self):\r\n        """Fuse sensor data."""\r\n        if not all([self.image_data, self.scan_data, self.imu_data]):\r\n            return\r\n        \r\n        # Extract information from each sensor\r\n        # Image: Object detection, visual features\r\n        # LiDAR: Distance measurements, obstacle detection\r\n        # IMU: Orientation, acceleration\r\n        \r\n        # Combine for robust perception\r\n        # Example: Use IMU to stabilize image processing\r\n        # Use LiDAR to validate visual detections\r\n        \r\n        self.get_logger().info(\'Sensor fusion complete\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SensorFusion()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"object-detection--slam--nav2-integration",children:"Object Detection + SLAM + Nav2 Integration"}),"\n",(0,o.jsx)(n.h3,{id:"complete-system",children:"Complete System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete Autonomous System\r\n\r\nObject detection, SLAM, and navigation integrated.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom nav_msgs.msg import OccupancyGrid\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom rclpy.action import ActionClient\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport math\r\n\r\nclass AutonomousSystem(Node):\r\n    def __init__(self):\r\n        super().__init__(\'autonomous_system\')\r\n        \r\n        # Components\r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        \r\n        # State\r\n        self.map = None\r\n        self.target_object = None\r\n        self.navigation_active = False\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        self.map_sub = self.create_subscription(\r\n            OccupancyGrid, \'/map\', self.map_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        # Timer\r\n        self.timer = self.create_timer(0.1, self.autonomous_loop)\r\n        \r\n        self.get_logger().info(\'Autonomous system started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Process images for object detection."""\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            results = self.yolo_model(cv_image)\r\n            \r\n            # Look for target object (e.g., "person")\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    cls = int(box.cls[0])\r\n                    label = self.yolo_model.names[cls]\r\n                    \r\n                    if label == "person" and float(box.conf[0]) > 0.5:\r\n                        # Calculate object position in image\r\n                        bbox = box.xyxy[0].cpu().numpy()\r\n                        center_x = (bbox[0] + bbox[2]) / 2\r\n                        center_y = (bbox[1] + bbox[3]) / 2\r\n                        \r\n                        # Convert to world coordinates (simplified)\r\n                        # In real system, use camera calibration and depth\r\n                        self.target_object = {\r\n                            \'label\': label,\r\n                            \'image_x\': center_x,\r\n                            \'image_y\': center_y\r\n                        }\r\n                        self.get_logger().info(f\'Target detected: {label}\')\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Image processing error: {e}\')\r\n    \r\n    def map_callback(self, msg):\r\n        """Store map."""\r\n        self.map = msg\r\n    \r\n    def navigate_to_object(self, x, y):\r\n        """Navigate to detected object."""\r\n        if self.navigation_active:\r\n            return\r\n        \r\n        goal = NavigateToPose.Goal()\r\n        goal.pose.header.frame_id = \'map\'\r\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal.pose.pose.position.x = x\r\n        goal.pose.pose.position.y = y\r\n        goal.pose.pose.orientation.w = 1.0\r\n        \r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n        \r\n        goal_handle = future.result()\r\n        if goal_handle.accepted:\r\n            self.navigation_active = True\r\n            self.get_logger().info(\'Navigation to object started\')\r\n    \r\n    def autonomous_loop(self):\r\n        """Main autonomous behavior loop."""\r\n        if self.target_object and not self.navigation_active:\r\n            # Simple strategy: navigate towards object\r\n            # In real system, use more sophisticated approach\r\n            # For now, just log\r\n            self.get_logger().info(\'Planning navigation to target object\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = AutonomousSystem()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Reduce Image Resolution"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Downscale for faster processing\r\nsmall_image = cv2.resize(cv_image, (320, 240))\r\nresults = self.yolo_model(small_image)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Process at Lower Frequency"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Process every 5th frame\r\nself.frame_count = 0\r\nif self.frame_count % 5 == 0:\r\n    # Process image\r\n    pass\r\nself.frame_count += 1\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Use Smaller Models"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Use nano model for speed\r\nmodel = YOLO('yolov8n.pt')  # Fastest\r\n# Instead of\r\n# model = YOLO('yolov8x.pt')  # Slowest but most accurate\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Multithreading"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import threading\r\n\r\nclass OptimizedPipeline(Node):\r\n    def __init__(self):\r\n        super().__init__(\'optimized_pipeline\')\r\n        self.processing_thread = threading.Thread(target=self.process_images)\r\n        self.processing_thread.start()\r\n    \r\n    def process_images(self):\r\n        """Process images in separate thread."""\r\n        while rclpy.ok():\r\n            if self.image_queue:\r\n                image = self.image_queue.pop(0)\r\n                # Process image\r\n                pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"deployment-tips",children:"Deployment Tips"}),"\n",(0,o.jsx)(n.h3,{id:"1-resource-management",children:"1. Resource Management"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Monitor CPU and memory\r\nimport psutil\r\n\r\ndef check_resources(self):\r\n    cpu_percent = psutil.cpu_percent()\r\n    memory_percent = psutil.virtual_memory().percent\r\n    \r\n    if cpu_percent > 80:\r\n        self.get_logger().warn('High CPU usage')\r\n    if memory_percent > 80:\r\n        self.get_logger().warn('High memory usage')\n"})}),"\n",(0,o.jsx)(n.h3,{id:"2-error-recovery",children:"2. Error Recovery"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def robust_processing(self):\r\n    """Robust processing with error recovery."""\r\n    try:\r\n        # Main processing\r\n        result = self.process()\r\n    except Exception as e:\r\n        self.get_logger().error(f\'Error: {e}\')\r\n        # Fallback behavior\r\n        result = self.fallback_processing()\r\n    return result\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-logging-and-monitoring",children:"3. Logging and Monitoring"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Structured logging\r\nself.get_logger().info(\r\n    f'Processing frame: detection_count={len(detections)}, '\r\n    f'processing_time={processing_time:.3f}s'\r\n)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"4-configuration-files",children:"4. Configuration Files"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# config.yaml\r\nperception:\r\n  yolo_model: "yolov8n.pt"\r\n  confidence_threshold: 0.5\r\n  image_resolution: [640, 480]\r\n  \r\nnavigation:\r\n  planner: "navfn"\r\n  controller: "dwb"\r\n  inflation_radius: 0.55\n'})}),"\n",(0,o.jsx)(n.h2,{id:"complete-launch-file",children:"Complete Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Perception pipeline\r\n        Node(\r\n            package='perception_package',\r\n            executable='perception_pipeline',\r\n            name='perception_pipeline'\r\n        ),\r\n        \r\n        # SLAM (if using RTAB-Map)\r\n        Node(\r\n            package='rtabmap_ros',\r\n            executable='rtabmap',\r\n            name='rtabmap'\r\n        ),\r\n        \r\n        # Navigation\r\n        Node(\r\n            package='nav2_bringup',\r\n            executable='bringup',\r\n            name='nav2_bringup'\r\n        ),\r\n        \r\n        # Robot control\r\n        Node(\r\n            package='robot_control',\r\n            executable='autonomous_controller',\r\n            name='autonomous_controller'\r\n        )\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsx)(n.h3,{id:"test-checklist",children:"Test Checklist"}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection working"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","SLAM building maps"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Navigation planning paths"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor fusion combining data"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System handles errors gracefully"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance meets requirements"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All components communicate correctly"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"module-3-summary",children:"Module 3 Summary"}),"\n",(0,o.jsx)(n.p,{children:"Congratulations! You've completed Module 3: AI-Powered Perception. You now understand:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u2705 Free alternatives to expensive tools"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 PyBullet for simulation"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 OpenCV for computer vision"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 ORB-SLAM3 for SLAM"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Nav2 for navigation"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Complete perception pipelines"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"All tools are FREE and work on normal laptops!"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"Continue your learning:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Module 4: Vision-Language-Action"})," - Natural language control"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Projects"})," - Apply your knowledge"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Keep building amazing robots for free! \ud83e\udd16"})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var s=r(6540);const o={},i=s.createContext(o);function t(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);