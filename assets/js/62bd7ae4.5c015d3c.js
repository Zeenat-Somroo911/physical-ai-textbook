"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[726],{7174:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-04-vla/chapter-03-language-grounding","title":"Chapter 3: Language Grounding","description":"Introduction","source":"@site/docs/module-04-vla/chapter-03-language-grounding.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/chapter-03-language-grounding","permalink":"/physical-ai-textbook/docs/module-04-vla/chapter-03-language-grounding","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/chapter-03-language-grounding.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4}}');var s=i(4848),t=i(8453);const a={sidebar_position:4},o="Chapter 3: Language Grounding",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"What is Language Grounding?",id:"what-is-language-grounding",level:2},{value:"Object Grounding",id:"object-grounding",level:2},{value:"Visual Object Detection",id:"visual-object-detection",level:3},{value:"Referring Expression Comprehension",id:"referring-expression-comprehension",level:3},{value:"Spatial Grounding",id:"spatial-grounding",level:2},{value:"Spatial Relationship Understanding",id:"spatial-relationship-understanding",level:3},{value:"Spatial Attention",id:"spatial-attention",level:3},{value:"Action Grounding",id:"action-grounding",level:2},{value:"Action-Object Mapping",id:"action-object-mapping",level:3},{value:"Temporal Grounding",id:"temporal-grounding",level:2},{value:"Sequence Understanding",id:"sequence-understanding",level:3},{value:"Grounding Datasets",id:"grounding-datasets",level:2},{value:"Popular Datasets",id:"popular-datasets",level:3},{value:"Training Grounding Models",id:"training-grounding-models",level:2},{value:"Weakly Supervised Learning",id:"weakly-supervised-learning",level:3},{value:"Contrastive Learning",id:"contrastive-learning",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-3-language-grounding",children:"Chapter 3: Language Grounding"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Language grounding connects natural language instructions to visual scenes, enabling robots to understand what to do and where to do it."}),"\n",(0,s.jsx)(n.h2,{id:"what-is-language-grounding",children:"What is Language Grounding?"}),"\n",(0,s.jsx)(n.p,{children:"Language grounding involves:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object grounding"}),": Identifying objects mentioned in language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spatial grounding"}),": Understanding spatial relationships"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action grounding"}),": Mapping language to actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal grounding"}),": Understanding sequences and timing"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"object-grounding",children:"Object Grounding"}),"\n",(0,s.jsx)(n.h3,{id:"visual-object-detection",children:"Visual Object Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\n\r\nclass ObjectGrounding(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vision_encoder = VisionEncoder()\r\n        self.language_encoder = LanguageEncoder()\r\n        self.object_detector = ObjectDetector()\r\n    \r\n    def forward(self, image, text):\r\n        # Extract object mentions from text\r\n        objects = extract_objects(text)  # ["cup", "table"]\r\n        \r\n        # Detect objects in image\r\n        detections = self.object_detector(image)\r\n        \r\n        # Match language to visual objects\r\n        matches = match_objects(objects, detections)\r\n        return matches\n'})}),"\n",(0,s.jsx)(n.h3,{id:"referring-expression-comprehension",children:"Referring Expression Comprehension"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ReferringExpression(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = MultimodalEncoder()\r\n        self.scorer = nn.Linear(hidden_dim, 1)\r\n    \r\n    def forward(self, image, expression):\r\n        # "the red cup on the table"\r\n        features = self.encoder(image, expression)\r\n        \r\n        # Score all regions\r\n        scores = self.scorer(features)\r\n        best_region = torch.argmax(scores)\r\n        return best_region\n'})}),"\n",(0,s.jsx)(n.h2,{id:"spatial-grounding",children:"Spatial Grounding"}),"\n",(0,s.jsx)(n.h3,{id:"spatial-relationship-understanding",children:"Spatial Relationship Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SpatialGrounding(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = MultimodalEncoder()\r\n        self.spatial_parser = SpatialParser()\r\n    \r\n    def forward(self, image, instruction):\r\n        # "pick up the cup on the left side of the table"\r\n        features = self.encoder(image, instruction)\r\n        \r\n        # Parse spatial relationships\r\n        relationships = self.spatial_parser(features)\r\n        # Returns: {cup: {position: left, relative_to: table}}\r\n        return relationships\n'})}),"\n",(0,s.jsx)(n.h3,{id:"spatial-attention",children:"Spatial Attention"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SpatialAttention(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.attention = nn.MultiheadAttention(dim, num_heads=8)\r\n    \r\n    def forward(self, image_features, spatial_query):\r\n        # spatial_query: "left", "right", "above", etc.\r\n        attended, weights = self.attention(\r\n            image_features, spatial_query, spatial_query\r\n        )\r\n        return attended, weights\n'})}),"\n",(0,s.jsx)(n.h2,{id:"action-grounding",children:"Action Grounding"}),"\n",(0,s.jsx)(n.h3,{id:"action-object-mapping",children:"Action-Object Mapping"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ActionGrounding(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.action_classifier = nn.Linear(hidden_dim, num_actions)\r\n        self.object_localizer = ObjectLocalizer()\r\n    \r\n    def forward(self, image, instruction):\r\n        # "pick up the cup"\r\n        # Extract action: "pick up"\r\n        # Extract object: "cup"\r\n        \r\n        action = classify_action(instruction)\r\n        object_location = self.object_localizer(image, "cup")\r\n        \r\n        return {\r\n            \'action\': action,\r\n            \'object\': object_location,\r\n            \'target_pose\': compute_grasp_pose(object_location)\r\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"temporal-grounding",children:"Temporal Grounding"}),"\n",(0,s.jsx)(n.h3,{id:"sequence-understanding",children:"Sequence Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TemporalGrounding(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2)\r\n        self.temporal_parser = TemporalParser()\r\n    \r\n    def forward(self, video_frames, instruction):\r\n        # "first pick up the cup, then place it on the table"\r\n        features = [self.encoder(frame, instruction) for frame in video_frames]\r\n        \r\n        # Understand temporal sequence\r\n        sequence, _ = self.lstm(torch.stack(features))\r\n        parsed = self.temporal_parser(sequence)\r\n        return parsed\n'})}),"\n",(0,s.jsx)(n.h2,{id:"grounding-datasets",children:"Grounding Datasets"}),"\n",(0,s.jsx)(n.h3,{id:"popular-datasets",children:"Popular Datasets"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RefCOCO/RefCOCO+"}),": Referring expressions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flickr30k Entities"}),": Visual grounding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual Genome"}),": Scene understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ALFRED"}),": Language-conditioned tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"training-grounding-models",children:"Training Grounding Models"}),"\n",(0,s.jsx)(n.h3,{id:"weakly-supervised-learning",children:"Weakly Supervised Learning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def grounding_loss(predictions, weak_labels):\r\n    # Weak labels: object present/not present\r\n    # No exact bounding boxes\r\n    loss = F.binary_cross_entropy(predictions, weak_labels)\r\n    return loss\n"})}),"\n",(0,s.jsx)(n.h3,{id:"contrastive-learning",children:"Contrastive Learning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def contrastive_grounding_loss(positive_pairs, negative_pairs):\r\n    # Positive: (image, correct_text)\r\n    # Negative: (image, incorrect_text)\r\n    \r\n    pos_sim = cosine_similarity(positive_pairs)\r\n    neg_sim = cosine_similarity(negative_pairs)\r\n    \r\n    loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(neg_sim)))\r\n    return loss\n"})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accuracy"}),": Correct object identification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IoU"}),": Intersection over Union for bounding boxes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Success Rate"}),": Task completion rate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grounding Precision"}),": Correct spatial grounding"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use pre-trained vision-language models"}),"\n",(0,s.jsx)(n.li,{children:"Leverage spatial reasoning modules"}),"\n",(0,s.jsx)(n.li,{children:"Train with diverse instructions"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate on real-world scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement object grounding"}),"\n",(0,s.jsx)(n.li,{children:"Add spatial relationship parsing"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate on grounding dataset"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Chapter 4 covers action prediction from language and vision."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);