"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[5798],{1573:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"projects/project-03-perception","title":"Project 3: Autonomous Navigation System","description":"Project Overview","source":"@site/docs/projects/project-03-perception.md","sourceDirName":"projects","slug":"/projects/project-03-perception","permalink":"/physical-ai-textbook/docs/projects/project-03-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/projects/project-03-perception.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Project 2: Build Your Own Humanoid","permalink":"/physical-ai-textbook/docs/projects/project-02-simulation"},"next":{"title":"Project 4: Voice-Controlled Robot Butler","permalink":"/physical-ai-textbook/docs/projects/project-04-vla"}}');var i=r(4848),o=r(8453);const s={sidebar_position:4},a="Project 3: Autonomous Navigation System",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Functional Requirements",id:"functional-requirements",level:3},{value:"Technical Requirements",id:"technical-requirements",level:3},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create ROS 2 Package",id:"step-1-create-ros-2-package",level:3},{value:"Step 2: Object Detection Node",id:"step-2-object-detection-node",level:3},{value:"Step 3: SLAM Node",id:"step-3-slam-node",level:3},{value:"Step 4: Navigation Integration Node",id:"step-4-navigation-integration-node",level:3},{value:"Step 5: Launch File",id:"step-5-launch-file",level:3},{value:"Testing Steps",id:"testing-steps",level:2},{value:"Test 1: Object Detection",id:"test-1-object-detection",level:3},{value:"Test 2: SLAM",id:"test-2-slam",level:3},{value:"Test 3: Navigation",id:"test-3-navigation",level:3},{value:"Test 4: Complete System",id:"test-4-complete-system",level:3},{value:"Expected Outputs",id:"expected-outputs",level:2},{value:"Object Detection",id:"object-detection",level:3},{value:"SLAM",id:"slam",level:3},{value:"Navigation",id:"navigation",level:3},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Object Detection (30 points)",id:"object-detection-30-points",level:3},{value:"SLAM (30 points)",id:"slam-30-points",level:3},{value:"Navigation (30 points)",id:"navigation-30-points",level:3},{value:"Integration (10 points)",id:"integration-10-points",level:3},{value:"Extensions",id:"extensions",level:2},{value:"Extension 1: Advanced SLAM",id:"extension-1-advanced-slam",level:3},{value:"Extension 2: Dynamic Obstacles",id:"extension-2-dynamic-obstacles",level:3},{value:"Extension 3: Semantic Navigation",id:"extension-3-semantic-navigation",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue: Objects not detected",id:"issue-objects-not-detected",level:3},{value:"Issue: Map not building",id:"issue-map-not-building",level:3},{value:"Issue: Navigation fails",id:"issue-navigation-fails",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"project-3-autonomous-navigation-system",children:"Project 3: Autonomous Navigation System"})}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"Build a complete autonomous navigation system that combines object detection, SLAM, and Nav2 for a robot to navigate while avoiding obstacles and reaching goals."}),"\n",(0,i.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate computer vision with ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Implement SLAM for map building"}),"\n",(0,i.jsx)(n.li,{children:"Use Nav2 for path planning"}),"\n",(0,i.jsx)(n.li,{children:"Combine perception and navigation"}),"\n",(0,i.jsx)(n.li,{children:"Build a complete autonomous system"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Completed ",(0,i.jsx)(n.a,{href:"../module-03-isaac/free-alternatives",children:"Module 3: AI-Powered Perception"})]}),"\n",(0,i.jsx)(n.li,{children:"Understanding of OpenCV"}),"\n",(0,i.jsx)(n.li,{children:"Basic knowledge of SLAM"}),"\n",(0,i.jsx)(n.li,{children:"Nav2 installed"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"functional-requirements",children:"Functional Requirements"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Detection"}),": Detect and classify objects in environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SLAM"}),": Build map while localizing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation"}),": Navigate to goals using Nav2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Obstacle Avoidance"}),": Avoid detected objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": All components working together"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"YOLO for object detection"}),"\n",(0,i.jsx)(n.li,{children:"RTAB-Map or ORB-SLAM3 for SLAM"}),"\n",(0,i.jsx)(n.li,{children:"Nav2 stack for navigation"}),"\n",(0,i.jsx)(n.li,{children:"ROS 2 integration"}),"\n",(0,i.jsx)(n.li,{children:"Real-time processing"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-create-ros-2-package",children:"Step 1: Create ROS 2 Package"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\r\nros2 pkg create --build-type ament_python autonomous_nav \\\r\n    --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs cv_bridge\r\ncd ~/ros2_ws\r\ncolcon build --packages-select autonomous_nav\r\nsource install/setup.bash\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-object-detection-node",children:"Step 2: Object Detection Node"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"autonomous_nav/autonomous_nav/object_detector.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nObject Detection Node\r\n\r\nDetects objects using YOLO and publishes detections.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport json\r\nimport cv2\r\n\r\nclass ObjectDetectorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('object_detector_node')\r\n        \r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO('yolov8n.pt')\r\n        \r\n        # Subscriber\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(\r\n            String,\r\n            '/detections',\r\n            10\r\n        )\r\n        \r\n        self.annotated_pub = self.create_publisher(\r\n            Image,\r\n            '/detections/image_annotated',\r\n            10\r\n        )\r\n        \r\n        self.get_logger().info('Object detector node started')\r\n    \r\n    def image_callback(self, msg):\r\n        \"\"\"Process images and detect objects.\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            \r\n            # Run YOLO detection\r\n            results = self.yolo_model(cv_image)\r\n            \r\n            detections = []\r\n            annotated_image = cv_image.copy()\r\n            \r\n            for result in results:\r\n                for box in result.boxes:\r\n                    if float(box.conf[0]) > 0.5:\r\n                        label = self.yolo_model.names[int(box.cls[0])]\r\n                        confidence = float(box.conf[0])\r\n                        bbox = box.xyxy[0].cpu().numpy().tolist()\r\n                        \r\n                        detections.append({\r\n                            'label': label,\r\n                            'confidence': confidence,\r\n                            'bbox': bbox\r\n                        })\r\n                        \r\n                        # Draw bounding box\r\n                        x1, y1, x2, y2 = map(int, bbox)\r\n                        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\r\n                        cv2.putText(\r\n                            annotated_image,\r\n                            f'{label}: {confidence:.2f}',\r\n                            (x1, y1 - 10),\r\n                            cv2.FONT_HERSHEY_SIMPLEX,\r\n                            0.5,\r\n                            (0, 255, 0),\r\n                            2\r\n                        )\r\n            \r\n            # Publish detections\r\n            detection_msg = String()\r\n            detection_msg.data = json.dumps(detections)\r\n            self.detection_pub.publish(detection_msg)\r\n            \r\n            # Publish annotated image\r\n            annotated_msg = self.bridge.cv2_to_imgmsg(annotated_image, \"bgr8\")\r\n            annotated_msg.header = msg.header\r\n            self.annotated_pub.publish(annotated_msg)\r\n            \r\n            if detections:\r\n                self.get_logger().info(f'Detected {len(detections)} objects')\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f'Detection error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ObjectDetectorNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-slam-node",children:"Step 3: SLAM Node"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"autonomous_nav/autonomous_nav/slam_node.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nSLAM Node\r\n\r\nSimplified SLAM using visual odometry and map building.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom nav_msgs.msg import OccupancyGrid, MapMetaData\r\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\r\nimport numpy as np\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\n\r\nclass SLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__('slam_node')\r\n        \r\n        self.bridge = CvBridge()\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.map_pub = self.create_publisher(\r\n            OccupancyGrid,\r\n            '/map',\r\n            10\r\n        )\r\n        \r\n        self.pose_pub = self.create_publisher(\r\n            PoseWithCovarianceStamped,\r\n            '/slam_pose',\r\n            10\r\n        )\r\n        \r\n        # Map initialization\r\n        self.map_resolution = 0.05  # 5cm per pixel\r\n        self.map_width = 400\r\n        self.map_height = 400\r\n        self.map_origin_x = -10.0\r\n        self.map_origin_y = -10.0\r\n        \r\n        self.map_data = np.full(\r\n            (self.map_height, self.map_width),\r\n            -1,\r\n            dtype=np.int8\r\n        )\r\n        \r\n        # Visual odometry\r\n        self.prev_image = None\r\n        self.robot_pose = [0.0, 0.0, 0.0]  # x, y, theta\r\n        \r\n        # Timer for map publishing\r\n        self.timer = self.create_timer(1.0, self.publish_map)\r\n        \r\n        self.get_logger().info('SLAM node started')\r\n    \r\n    def image_callback(self, msg):\r\n        \"\"\"Process images for visual odometry.\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\r\n            \r\n            if self.prev_image is not None:\r\n                # Feature detection and matching\r\n                orb = cv2.ORB_create()\r\n                kp1, des1 = orb.detectAndCompute(self.prev_image, None)\r\n                kp2, des2 = orb.detectAndCompute(gray, None)\r\n                \r\n                if des1 is not None and des2 is not None:\r\n                    # Match features\r\n                    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\r\n                    matches = bf.match(des1, des2)\r\n                    matches = sorted(matches, key=lambda x: x.distance)\r\n                    \r\n                    if len(matches) > 10:\r\n                        # Estimate motion (simplified)\r\n                        # In real implementation, use proper visual odometry\r\n                        dx = 0.1  # Simplified\r\n                        dy = 0.0\r\n                        dtheta = 0.0\r\n                        \r\n                        # Update pose\r\n                        self.robot_pose[0] += dx * np.cos(self.robot_pose[2]) - dy * np.sin(self.robot_pose[2])\r\n                        self.robot_pose[1] += dx * np.sin(self.robot_pose[2]) + dy * np.cos(self.robot_pose[2])\r\n                        self.robot_pose[2] += dtheta\r\n                        \r\n                        # Update map (simplified - mark current position as free)\r\n                        map_x = int((self.robot_pose[0] - self.map_origin_x) / self.map_resolution)\r\n                        map_y = int((self.robot_pose[1] - self.map_origin_y) / self.map_resolution)\r\n                        \r\n                        if 0 <= map_x < self.map_width and 0 <= map_y < self.map_height:\r\n                            self.map_data[map_y, map_x] = 0  # Free space\r\n            \r\n            self.prev_image = gray\r\n            \r\n            # Publish pose\r\n            pose_msg = PoseWithCovarianceStamped()\r\n            pose_msg.header.stamp = msg.header.stamp\r\n            pose_msg.header.frame_id = 'map'\r\n            pose_msg.pose.pose.position.x = self.robot_pose[0]\r\n            pose_msg.pose.pose.position.y = self.robot_pose[1]\r\n            pose_msg.pose.pose.orientation.z = np.sin(self.robot_pose[2] / 2.0)\r\n            pose_msg.pose.pose.orientation.w = np.cos(self.robot_pose[2] / 2.0)\r\n            self.pose_pub.publish(pose_msg)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f'SLAM error: {e}')\r\n    \r\n    def publish_map(self):\r\n        \"\"\"Publish occupancy grid map.\"\"\"\r\n        map_msg = OccupancyGrid()\r\n        map_msg.header.stamp = self.get_clock().now().to_msg()\r\n        map_msg.header.frame_id = 'map'\r\n        \r\n        map_msg.info.resolution = self.map_resolution\r\n        map_msg.info.width = self.map_width\r\n        map_msg.info.height = self.map_height\r\n        map_msg.info.origin.position.x = self.map_origin_x\r\n        map_msg.info.origin.position.y = self.map_origin_y\r\n        map_msg.info.origin.orientation.w = 1.0\r\n        \r\n        map_msg.data = self.map_data.flatten().tolist()\r\n        \r\n        self.map_pub.publish(map_msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SLAMNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-navigation-integration-node",children:"Step 4: Navigation Integration Node"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"autonomous_nav/autonomous_nav/navigation_integration.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nNavigation Integration Node\r\n\r\nIntegrates object detection, SLAM, and Nav2.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\nimport json\r\nimport math\r\n\r\nclass NavigationIntegrationNode(Node):\r\n    def __init__(self):\r\n        super().__init__('navigation_integration_node')\r\n        \r\n        # Subscribers\r\n        self.detection_sub = self.create_subscription(\r\n            String,\r\n            '/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n        \r\n        # Action client for Nav2\r\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\r\n        \r\n        # State\r\n        self.detected_objects = []\r\n        self.current_goal = None\r\n        \r\n        # Timer for goal management\r\n        self.timer = self.create_timer(1.0, self.manage_navigation)\r\n        \r\n        self.get_logger().info('Navigation integration node started')\r\n    \r\n    def detection_callback(self, msg):\r\n        \"\"\"Handle object detections.\"\"\"\r\n        try:\r\n            self.detected_objects = json.loads(msg.data)\r\n            \r\n            # Check for obstacles in path\r\n            for obj in self.detected_objects:\r\n                if obj['label'] in ['person', 'car', 'bicycle']:\r\n                    self.get_logger().warn(f'Obstacle detected: {obj[\"label\"]}')\r\n                    # In real implementation, replan path\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f'Detection callback error: {e}')\r\n    \r\n    def navigate_to_goal(self, x, y, theta=0.0):\r\n        \"\"\"Navigate to goal position.\"\"\"\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = 'map'\r\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal_msg.pose.pose.position.x = x\r\n        goal_msg.pose.pose.position.y = y\r\n        goal_msg.pose.pose.orientation.z = math.sin(theta / 2.0)\r\n        goal_msg.pose.pose.orientation.w = math.cos(theta / 2.0)\r\n        \r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal_msg)\r\n        \r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        if goal_handle.accepted:\r\n            self.get_logger().info(f'Navigating to ({x}, {y})')\r\n            return True\r\n        else:\r\n            self.get_logger().error('Goal rejected')\r\n            return False\r\n    \r\n    def manage_navigation(self):\r\n        \"\"\"Manage navigation goals.\"\"\"\r\n        # Example: Navigate to waypoints\r\n        if self.current_goal is None:\r\n            # Set first goal\r\n            self.navigate_to_goal(2.0, 2.0, 0.0)\r\n            self.current_goal = (2.0, 2.0)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = NavigationIntegrationNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-launch-file",children:"Step 5: Launch File"}),"\n",(0,i.jsxs)(n.p,{children:["Create ",(0,i.jsx)(n.code,{children:"autonomous_nav/launch/autonomous_nav.launch.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Object detector\r\n        Node(\r\n            package='autonomous_nav',\r\n            executable='object_detector',\r\n            name='object_detector',\r\n            output='screen'\r\n        ),\r\n        \r\n        # SLAM node\r\n        Node(\r\n            package='autonomous_nav',\r\n            executable='slam_node',\r\n            name='slam_node',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Navigation integration\r\n        Node(\r\n            package='autonomous_nav',\r\n            executable='navigation_integration',\r\n            name='navigation_integration',\r\n            output='screen'\r\n        ),\r\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-steps",children:"Testing Steps"}),"\n",(0,i.jsx)(n.h3,{id:"test-1-object-detection",children:"Test 1: Object Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Start object detector\r\nros2 run autonomous_nav object_detector\r\n\r\n# Check detections\r\nros2 topic echo /detections\r\n\r\n# View annotated images\r\nros2 run rviz2 rviz2\r\n# Add Image display for /detections/image_annotated\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),": Objects detected and annotated in images."]}),"\n",(0,i.jsx)(n.h3,{id:"test-2-slam",children:"Test 2: SLAM"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Start SLAM node\r\nros2 run autonomous_nav slam_node\r\n\r\n# Check map\r\nros2 topic echo /map\r\n\r\n# Check pose\r\nros2 topic echo /slam_pose\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),": Map building, pose estimates published."]}),"\n",(0,i.jsx)(n.h3,{id:"test-3-navigation",children:"Test 3: Navigation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Start Nav2 (if available)\r\nros2 launch nav2_bringup nav2_bringup_launch.py\r\n\r\n# Start integration node\r\nros2 run autonomous_nav navigation_integration\r\n\r\n# Send goal\r\nros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose \"{pose: {header: {frame_id: 'map'}, pose: {position: {x: 2.0, y: 2.0}}}}\"\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),": Robot navigates to goal while avoiding obstacles."]}),"\n",(0,i.jsx)(n.h3,{id:"test-4-complete-system",children:"Test 4: Complete System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Launch all components\r\nros2 launch autonomous_nav autonomous_nav.launch.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),": All components working together, robot navigating autonomously."]}),"\n",(0,i.jsx)(n.h2,{id:"expected-outputs",children:"Expected Outputs"}),"\n",(0,i.jsx)(n.h3,{id:"object-detection",children:"Object Detection"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Bounding boxes around detected objects"}),"\n",(0,i.jsx)(n.li,{children:"Labels and confidence scores"}),"\n",(0,i.jsx)(n.li,{children:"Annotated images published"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"slam",children:"SLAM"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Occupancy grid map"}),"\n",(0,i.jsx)(n.li,{children:"Robot pose estimates"}),"\n",(0,i.jsx)(n.li,{children:"Map updating as robot moves"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"navigation",children:"Navigation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Path planning to goals"}),"\n",(0,i.jsx)(n.li,{children:"Obstacle avoidance"}),"\n",(0,i.jsx)(n.li,{children:"Goal reaching"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,i.jsx)(n.h3,{id:"object-detection-30-points",children:"Object Detection (30 points)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"YOLO Integration"})," (10 points): YOLO working correctly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Detection Publishing"})," (10 points): Detections published as ROS messages"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visualization"})," (10 points): Annotated images displayed"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"slam-30-points",children:"SLAM (30 points)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Map Building"})," (15 points): Map being built"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization"})," (15 points): Pose estimates accurate"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"navigation-30-points",children:"Navigation (30 points)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nav2 Integration"})," (15 points): Nav2 working"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Path Planning"})," (15 points): Paths planned correctly"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"integration-10-points",children:"Integration (10 points)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Integration"})," (5 points): All components working together"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance"})," (5 points): Real-time operation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,i.jsx)(n.h3,{id:"extension-1-advanced-slam",children:"Extension 1: Advanced SLAM"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use RTAB-Map or ORB-SLAM3"}),"\n",(0,i.jsx)(n.li,{children:"Loop closure detection"}),"\n",(0,i.jsx)(n.li,{children:"3D mapping"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"extension-2-dynamic-obstacles",children:"Extension 2: Dynamic Obstacles"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Track moving objects"}),"\n",(0,i.jsx)(n.li,{children:"Predict trajectories"}),"\n",(0,i.jsx)(n.li,{children:"Dynamic replanning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"extension-3-semantic-navigation",children:"Extension 3: Semantic Navigation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Navigate to specific objects"}),"\n",(0,i.jsx)(n.li,{children:"Room-level navigation"}),"\n",(0,i.jsx)(n.li,{children:"Task-based navigation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"issue-objects-not-detected",children:"Issue: Objects not detected"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Check camera topic, verify YOLO model loaded, check image format."]}),"\n",(0,i.jsx)(n.h3,{id:"issue-map-not-building",children:"Issue: Map not building"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Verify camera data, check SLAM algorithm, verify pose updates."]}),"\n",(0,i.jsx)(n.h3,{id:"issue-navigation-fails",children:"Issue: Navigation fails"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Check Nav2 configuration, verify map available, check goal validity."]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/physical-ai-textbook/docs/projects/project-04-vla",children:"Project 4: Voice-Controlled Robot Butler"})," - VLA project"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"../module-04-vla/vla-introduction",children:"Module 4: Vision-Language-Action"})," - Learn VLA"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Excellent work!"})," You've built a complete autonomous navigation system! \ud83d\ude80"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);