"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[328],{8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>l});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},8655:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"projects/project-04-vla","title":"Project 4: Voice-Controlled Robot Butler","description":"Project Overview","source":"@site/docs/projects/project-04-vla.md","sourceDirName":"projects","slug":"/projects/project-04-vla","permalink":"/physical-ai-textbook/docs/projects/project-04-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/projects/project-04-vla.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Project 3: Autonomous Navigation System","permalink":"/physical-ai-textbook/docs/projects/project-03-perception"},"next":{"title":"Final Project: Capstone Guidelines","permalink":"/physical-ai-textbook/docs/projects/final-project"}}');var o=r(4848),i=r(8453);const s={sidebar_position:5},l="Project 4: Voice-Controlled Robot Butler",c={},a=[{value:"Project Overview",id:"project-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Functional Requirements",id:"functional-requirements",level:3},{value:"Technical Requirements",id:"technical-requirements",level:3},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create ROS 2 Package",id:"step-1-create-ros-2-package",level:3},{value:"Step 2: Voice Recognition Node",id:"step-2-voice-recognition-node",level:3},{value:"Step 3: LLM Integration Node",id:"step-3-llm-integration-node",level:3},{value:"Step 4: Vision Processor Node",id:"step-4-vision-processor-node",level:3},{value:"Step 5: Action Executor Node",id:"step-5-action-executor-node",level:3},{value:"Step 6: Complete Launch File",id:"step-6-complete-launch-file",level:3},{value:"Testing Steps",id:"testing-steps",level:2},{value:"Test 1: Voice Recognition",id:"test-1-voice-recognition",level:3},{value:"Test 2: LLM Planning",id:"test-2-llm-planning",level:3},{value:"Test 3: Vision Processing",id:"test-3-vision-processing",level:3},{value:"Test 4: Complete System",id:"test-4-complete-system",level:3},{value:"Expected Outputs",id:"expected-outputs",level:2},{value:"Voice Recognition",id:"voice-recognition",level:3},{value:"LLM Planning",id:"llm-planning",level:3},{value:"Vision Processing",id:"vision-processing",level:3},{value:"Action Execution",id:"action-execution",level:3},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Voice Recognition (20 points)",id:"voice-recognition-20-points",level:3},{value:"LLM Integration (25 points)",id:"llm-integration-25-points",level:3},{value:"Vision Processing (20 points)",id:"vision-processing-20-points",level:3},{value:"Action Execution (25 points)",id:"action-execution-25-points",level:3},{value:"Integration (10 points)",id:"integration-10-points",level:3},{value:"Extensions",id:"extensions",level:2},{value:"Extension 1: Multimodal Understanding",id:"extension-1-multimodal-understanding",level:3},{value:"Extension 2: Learning",id:"extension-2-learning",level:3},{value:"Extension 3: Advanced Tasks",id:"extension-3-advanced-tasks",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue: Voice not recognized",id:"issue-voice-not-recognized",level:3},{value:"Issue: LLM not generating plans",id:"issue-llm-not-generating-plans",level:3},{value:"Issue: Actions not executing",id:"issue-actions-not-executing",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"project-4-voice-controlled-robot-butler",children:"Project 4: Voice-Controlled Robot Butler"})}),"\n",(0,o.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(n.p,{children:"Build a complete Voice-Language-Action (VLA) system that enables a robot to understand natural language commands, perceive its environment, and execute complex tasks autonomously."}),"\n",(0,o.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate voice recognition with ROS 2"}),"\n",(0,o.jsx)(n.li,{children:"Use LLMs for natural language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Implement action planning and execution"}),"\n",(0,o.jsx)(n.li,{children:"Combine vision, language, and action"}),"\n",(0,o.jsx)(n.li,{children:"Build a complete VLA system"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Completed ",(0,o.jsx)(n.a,{href:"../module-04-vla/vla-introduction",children:"Module 4: Vision-Language-Action"})]}),"\n",(0,o.jsx)(n.li,{children:"OpenAI API key ($5 credit)"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of multimodal systems"}),"\n",(0,o.jsx)(n.li,{children:"Basic knowledge of action planning"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,o.jsx)(n.h3,{id:"functional-requirements",children:"Functional Requirements"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Recognition"}),": Understand spoken commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language Understanding"}),": Parse and understand commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Planning"}),": Break down commands into actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision Integration"}),": Use camera for scene understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Execution"}),": Execute planned actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"technical-requirements",children:"Technical Requirements"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Web Speech API or Whisper for voice"}),"\n",(0,o.jsx)(n.li,{children:"OpenAI API for language understanding"}),"\n",(0,o.jsx)(n.li,{children:"ROS 2 for robot control"}),"\n",(0,o.jsx)(n.li,{children:"Computer vision for perception"}),"\n",(0,o.jsx)(n.li,{children:"Action planning system"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"step-1-create-ros-2-package",children:"Step 1: Create ROS 2 Package"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\r\nros2 pkg create --build-type ament_python robot_butler \\\r\n    --dependencies rclpy std_msgs sensor_msgs geometry_msgs\r\ncd ~/ros2_ws\r\ncolcon build --packages-select robot_butler\r\nsource install/setup.bash\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-2-voice-recognition-node",children:"Step 2: Voice Recognition Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"robot_butler/robot_butler/voice_recognizer.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVoice Recognition Node\r\n\r\nRecognizes voice commands using Web Speech API or Whisper.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport speech_recognition as sr\r\nimport threading\r\n\r\nclass VoiceRecognizerNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_recognizer_node')\r\n        \r\n        # Speech recognition\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\r\n        \r\n        # Publisher\r\n        self.command_pub = self.create_publisher(\r\n            String,\r\n            '/butler/voice_command',\r\n            10\r\n        )\r\n        \r\n        # Start listening thread\r\n        self.listening = True\r\n        self.listen_thread = threading.Thread(target=self.listen_loop)\r\n        self.listen_thread.start()\r\n        \r\n        self.get_logger().info('Voice recognizer node started')\r\n    \r\n    def listen_loop(self):\r\n        \"\"\"Continuous listening loop.\"\"\"\r\n        while self.listening and rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    audio = self.recognizer.listen(\r\n                        source,\r\n                        timeout=1,\r\n                        phrase_time_limit=5\r\n                    )\r\n                \r\n                try:\r\n                    # Use Google Speech Recognition (free)\r\n                    text = self.recognizer.recognize_google(audio).lower()\r\n                    self.get_logger().info(f'Recognized: {text}')\r\n                    \r\n                    # Publish command\r\n                    msg = String()\r\n                    msg.data = text\r\n                    self.command_pub.publish(msg)\r\n                \r\n                except sr.UnknownValueError:\r\n                    pass\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f'Recognition error: {e}')\r\n            \r\n            except sr.WaitTimeoutError:\r\n                pass\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceRecognizerNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.listening = False\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-3-llm-integration-node",children:"Step 3: LLM Integration Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"robot_butler/robot_butler/llm_integration.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLLM Integration Node\r\n\r\nProcesses commands using OpenAI API.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport openai\r\nimport os\r\nimport json\r\n\r\nclass LLMIntegrationNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_integration_node\')\r\n        \r\n        # Set OpenAI API key\r\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n        if not openai.api_key:\r\n            self.get_logger().error(\'OPENAI_API_KEY not set!\')\r\n        \r\n        # Subscriber\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/butler/voice_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher\r\n        self.plan_pub = self.create_publisher(\r\n            String,\r\n            \'/butler/action_plan\',\r\n            10\r\n        )\r\n        \r\n        self.get_logger().info(\'LLM integration node started\')\r\n    \r\n    def command_callback(self, msg):\r\n        """Process command using LLM."""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Processing command: {command}\')\r\n        \r\n        # Generate action plan using LLM\r\n        plan = self.generate_plan(command)\r\n        \r\n        if plan:\r\n            # Publish plan\r\n            plan_msg = String()\r\n            plan_msg.data = json.dumps(plan)\r\n            self.plan_pub.publish(plan_msg)\r\n    \r\n    def generate_plan(self, command):\r\n        """Generate action plan using OpenAI."""\r\n        prompt = f"""You are a robot butler. Break down this command into actions: "{command}"\r\n\r\nAvailable actions:\r\n- navigate_to(location)\r\n- pick_up(object)\r\n- place(object, location)\r\n- open(door)\r\n- close(door)\r\n- check(object)\r\n- wait(duration)\r\n\r\nReturn JSON array:\r\n[\r\n    {{"action": "action_name", "parameters": {{...}}, "description": "..."}},\r\n    ...\r\n]"""\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": "You are a robot butler planner. Return valid JSON arrays."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                temperature=0.3,\r\n                max_tokens=500\r\n            )\r\n            \r\n            text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            if \'```\' in text:\r\n                text = text.split(\'```\')[1]\r\n                if text.startswith(\'json\'):\r\n                    text = text[4:]\r\n            \r\n            plan = json.loads(text)\r\n            self.get_logger().info(f\'Plan generated: {len(plan)} steps\')\r\n            return plan\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'LLM error: {e}\')\r\n            return None\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LLMIntegrationNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"step-4-vision-processor-node",children:"Step 4: Vision Processor Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"robot_butler/robot_butler/vision_processor.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVision Processor Node\r\n\r\nProcesses camera images for scene understanding.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport json\r\n\r\nclass VisionProcessorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vision_processor_node')\r\n        \r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO('yolov8n.pt')\r\n        \r\n        # Subscriber\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher\r\n        self.scene_pub = self.create_publisher(\r\n            String,\r\n            '/butler/scene',\r\n            10\r\n        )\r\n        \r\n        self.detected_objects = []\r\n        \r\n        self.get_logger().info('Vision processor node started')\r\n    \r\n    def image_callback(self, msg):\r\n        \"\"\"Process images for scene understanding.\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            \r\n            # Downscale for speed\r\n            small_image = cv2.resize(cv_image, (320, 240))\r\n            \r\n            # Detect objects\r\n            results = self.yolo_model(small_image)\r\n            \r\n            self.detected_objects = []\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    if float(box.conf[0]) > 0.5:\r\n                        self.detected_objects.append({\r\n                            'label': self.yolo_model.names[int(box.cls[0])],\r\n                            'confidence': float(box.conf[0]),\r\n                            'bbox': box.xyxy[0].cpu().numpy().tolist()\r\n                        })\r\n            \r\n            # Publish scene description\r\n            scene_msg = String()\r\n            scene_msg.data = json.dumps(self.detected_objects)\r\n            self.scene_pub.publish(scene_msg)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f'Vision error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionProcessorNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-5-action-executor-node",children:"Step 5: Action Executor Node"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"robot_butler/robot_butler/action_executor.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nAction Executor Node\r\n\r\nExecutes planned actions.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\nimport json\r\nimport math\r\n\r\nclass ActionExecutorNode(Node):\r\n    def __init__(self):\r\n        super().__init__('action_executor_node')\r\n        \r\n        # Subscriber\r\n        self.plan_sub = self.create_subscription(\r\n            String,\r\n            '/butler/action_plan',\r\n            self.plan_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.status_pub = self.create_publisher(String, '/butler/status', 10)\r\n        \r\n        # Action client\r\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\r\n        \r\n        # State\r\n        self.current_plan = []\r\n        self.current_step = 0\r\n        self.executing = False\r\n        \r\n        # Timer\r\n        self.timer = self.create_timer(0.1, self.execution_loop)\r\n        \r\n        self.get_logger().info('Action executor node started')\r\n    \r\n    def plan_callback(self, msg):\r\n        \"\"\"Receive new plan.\"\"\"\r\n        try:\r\n            plan = json.loads(msg.data)\r\n            self.current_plan = plan\r\n            self.current_step = 0\r\n            self.executing = True\r\n            self.get_logger().info(f'New plan received: {len(plan)} steps')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Plan parsing error: {e}')\r\n    \r\n    def execution_loop(self):\r\n        \"\"\"Main execution loop.\"\"\"\r\n        if self.executing and self.current_step < len(self.current_plan):\r\n            action = self.current_plan[self.current_step]\r\n            self.execute_action(action)\r\n    \r\n    def execute_action(self, action):\r\n        \"\"\"Execute a single action.\"\"\"\r\n        action_name = action.get('action')\r\n        params = action.get('parameters', {})\r\n        description = action.get('description', '')\r\n        \r\n        self.get_logger().info(f'Step {self.current_step + 1}: {description}')\r\n        \r\n        success = False\r\n        \r\n        if action_name == 'navigate_to':\r\n            success = self.navigate_to(params)\r\n        elif action_name == 'pick_up':\r\n            success = self.pick_up(params)\r\n        elif action_name == 'place':\r\n            success = self.place(params)\r\n        elif action_name == 'stop':\r\n            success = self.stop()\r\n        else:\r\n            self.get_logger().warn(f'Unknown action: {action_name}')\r\n        \r\n        if success:\r\n            self.current_step += 1\r\n            \r\n            if self.current_step >= len(self.current_plan):\r\n                self.executing = False\r\n                self.get_logger().info('Plan completed!')\r\n                \r\n                status_msg = String()\r\n                status_msg.data = 'completed'\r\n                self.status_pub.publish(status_msg)\r\n    \r\n    def navigate_to(self, params):\r\n        \"\"\"Navigate to location.\"\"\"\r\n        x = params.get('x', 0.0)\r\n        y = params.get('y', 0.0)\r\n        theta = params.get('theta', 0.0)\r\n        \r\n        goal = NavigateToPose.Goal()\r\n        goal.pose.header.frame_id = 'map'\r\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal.pose.pose.position.x = x\r\n        goal.pose.pose.position.y = y\r\n        goal.pose.pose.orientation.z = math.sin(theta / 2.0)\r\n        goal.pose.pose.orientation.w = math.cos(theta / 2.0)\r\n        \r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n        \r\n        goal_handle = future.result()\r\n        return goal_handle.accepted\r\n    \r\n    def pick_up(self, params):\r\n        \"\"\"Pick up object.\"\"\"\r\n        object_name = params.get('object', 'unknown')\r\n        self.get_logger().info(f'Picking up {object_name}')\r\n        return True\r\n    \r\n    def place(self, params):\r\n        \"\"\"Place object.\"\"\"\r\n        object_name = params.get('object', 'unknown')\r\n        location = params.get('location', 'unknown')\r\n        self.get_logger().info(f'Placing {object_name} at {location}')\r\n        return True\r\n    \r\n    def stop(self):\r\n        \"\"\"Stop robot.\"\"\"\r\n        cmd = Twist()\r\n        self.cmd_vel_pub.publish(cmd)\r\n        return True\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ActionExecutorNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"step-6-complete-launch-file",children:"Step 6: Complete Launch File"}),"\n",(0,o.jsxs)(n.p,{children:["Create ",(0,o.jsx)(n.code,{children:"robot_butler/launch/butler.launch.py"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Voice recognizer\r\n        Node(\r\n            package='robot_butler',\r\n            executable='voice_recognizer',\r\n            name='voice_recognizer',\r\n            output='screen'\r\n        ),\r\n        \r\n        # LLM integration\r\n        Node(\r\n            package='robot_butler',\r\n            executable='llm_integration',\r\n            name='llm_integration',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Vision processor\r\n        Node(\r\n            package='robot_butler',\r\n            executable='vision_processor',\r\n            name='vision_processor',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Action executor\r\n        Node(\r\n            package='robot_butler',\r\n            executable='action_executor',\r\n            name='action_executor',\r\n            output='screen'\r\n        ),\r\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-steps",children:"Testing Steps"}),"\n",(0,o.jsx)(n.h3,{id:"test-1-voice-recognition",children:"Test 1: Voice Recognition"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Start voice recognizer\r\nros2 run robot_butler voice_recognizer\r\n\r\n# Speak commands\r\n# Check output\r\nros2 topic echo /butler/voice_command\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Output"}),": Spoken commands converted to text."]}),"\n",(0,o.jsx)(n.h3,{id:"test-2-llm-planning",children:"Test 2: LLM Planning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Set API key\r\nexport OPENAI_API_KEY='your-key-here'\r\n\r\n# Start LLM node\r\nros2 run robot_butler llm_integration\r\n\r\n# Send test command\r\nros2 topic pub /butler/voice_command std_msgs/msg/String \"{data: 'pick up the cup'}\"\r\n\r\n# Check plan\r\nros2 topic echo /butler/action_plan\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Output"}),": Action plan generated from command."]}),"\n",(0,o.jsx)(n.h3,{id:"test-3-vision-processing",children:"Test 3: Vision Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Start vision processor\r\nros2 run robot_butler vision_processor\r\n\r\n# Check scene\r\nros2 topic echo /butler/scene\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Output"}),": Objects detected and scene description published."]}),"\n",(0,o.jsx)(n.h3,{id:"test-4-complete-system",children:"Test 4: Complete System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Launch all components\r\nros2 launch robot_butler butler.launch.py\r\n\r\n# Speak: "Go to the kitchen and pick up the cup"\r\n# Watch robot execute the plan\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Expected Output"}),": Robot understands command, plans actions, and executes them."]}),"\n",(0,o.jsx)(n.h2,{id:"expected-outputs",children:"Expected Outputs"}),"\n",(0,o.jsx)(n.h3,{id:"voice-recognition",children:"Voice Recognition"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Commands recognized and published"}),"\n",(0,o.jsx)(n.li,{children:"Text output matches speech"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"llm-planning",children:"LLM Planning"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Commands broken down into actions"}),"\n",(0,o.jsx)(n.li,{children:"Valid JSON plan generated"}),"\n",(0,o.jsx)(n.li,{children:"Actions are executable"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"vision-processing",children:"Vision Processing"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Objects detected in scene"}),"\n",(0,o.jsx)(n.li,{children:"Scene description available"}),"\n",(0,o.jsx)(n.li,{children:"Real-time updates"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"action-execution",children:"Action Execution"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Actions executed in sequence"}),"\n",(0,o.jsx)(n.li,{children:"Status updates published"}),"\n",(0,o.jsx)(n.li,{children:"Tasks completed successfully"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,o.jsx)(n.h3,{id:"voice-recognition-20-points",children:"Voice Recognition (20 points)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Recognition Accuracy"})," (10 points): Commands recognized correctly"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS Integration"})," (10 points): Commands published to topics"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"llm-integration-25-points",children:"LLM Integration (25 points)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan Generation"})," (15 points): Valid plans generated"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Command Understanding"})," (10 points): Commands understood correctly"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"vision-processing-20-points",children:"Vision Processing (20 points)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection"})," (10 points): Objects detected"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scene Understanding"})," (10 points): Scene described accurately"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"action-execution-25-points",children:"Action Execution (25 points)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"})," (15 points): Actions executed correctly"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Completion"})," (10 points): Tasks completed successfully"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"integration-10-points",children:"Integration (10 points)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Integration"})," (5 points): All components working together"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance"})," (5 points): Real-time operation"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,o.jsx)(n.h3,{id:"extension-1-multimodal-understanding",children:"Extension 1: Multimodal Understanding"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Combine vision and language"}),"\n",(0,o.jsx)(n.li,{children:"Context-aware planning"}),"\n",(0,o.jsx)(n.li,{children:"Better object grounding"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"extension-2-learning",children:"Extension 2: Learning"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Learn from demonstrations"}),"\n",(0,o.jsx)(n.li,{children:"Improve with experience"}),"\n",(0,o.jsx)(n.li,{children:"Adapt to new scenarios"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"extension-3-advanced-tasks",children:"Extension 3: Advanced Tasks"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Complex multi-step tasks"}),"\n",(0,o.jsx)(n.li,{children:"Task prioritization"}),"\n",(0,o.jsx)(n.li,{children:"Error recovery"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"issue-voice-not-recognized",children:"Issue: Voice not recognized"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Check microphone, reduce noise, speak clearly."]}),"\n",(0,o.jsx)(n.h3,{id:"issue-llm-not-generating-plans",children:"Issue: LLM not generating plans"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Check API key, verify internet connection, check prompt format."]}),"\n",(0,o.jsx)(n.h3,{id:"issue-actions-not-executing",children:"Issue: Actions not executing"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Solution"}),": Verify action executor, check robot hardware, verify topics."]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/physical-ai-textbook/docs/projects/final-project",children:"Final Project: Capstone Guidelines"})," - Complete capstone"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"../module-04-vla/capstone-project",children:"Module 4: Capstone Project"})," - Detailed capstone guide"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Amazing work!"})," You've built a complete voice-controlled robot butler! \ud83c\udf89"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);