"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7567],{8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>l});var o=r(6540);const i={},t=o.createContext(i);function s(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),o.createElement(t.Provider,{value:e},n.children)}},9643:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-04-vla/voice-commands","title":"Chapter 2: Voice Commands","description":"Introduction","source":"@site/docs/module-04-vla/02-voice-commands.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/voice-commands","permalink":"/physical-ai-textbook/docs/module-04-vla/voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/02-voice-commands.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: VLA Introduction","permalink":"/physical-ai-textbook/docs/module-04-vla/vla-introduction"},"next":{"title":"Chapter 3: LLM Integration","permalink":"/physical-ai-textbook/docs/module-04-vla/llm-integration"}}');var i=r(4848),t=r(8453);const s={sidebar_position:2},l="Chapter 2: Voice Commands",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Why Voice Commands?",id:"why-voice-commands",level:3},{value:"Web Speech API (Free, Browser)",id:"web-speech-api-free-browser",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Basic Implementation",id:"basic-implementation",level:3},{value:"ROS 2 Bridge for Web Speech",id:"ros-2-bridge-for-web-speech",level:3},{value:"OpenAI Whisper API",id:"openai-whisper-api",level:2},{value:"Installation",id:"installation",level:3},{value:"Using Whisper API",id:"using-whisper-api",level:3},{value:"Local Whisper (Completely Free)",id:"local-whisper-completely-free",level:3},{value:"Real-Time Recognition",id:"real-time-recognition",level:2},{value:"Continuous Listening",id:"continuous-listening",level:3},{value:"Complete Voice System",id:"complete-voice-system",level:2},{value:"Full Integration",id:"full-integration",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Comparison",id:"comparison",level:3},{value:"Recommendations",id:"recommendations",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Common Errors and Solutions",id:"common-errors-and-solutions",level:2},{value:"Error 1: &quot;Microphone not found&quot;",id:"error-1-microphone-not-found",level:3},{value:"Error 2: &quot;Recognition timeout&quot;",id:"error-2-recognition-timeout",level:3},{value:"Error 3: &quot;API key not set&quot;",id:"error-3-api-key-not-set",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-2-voice-commands",children:"Chapter 2: Voice Commands"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(e.p,{children:["Voice commands enable natural, hands-free interaction with robots. This chapter covers ",(0,i.jsx)(e.strong,{children:"FREE"})," speech recognition solutions that work without expensive hardware or subscriptions."]}),"\n",(0,i.jsx)(e.h3,{id:"why-voice-commands",children:"Why Voice Commands?"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Natural Interaction"}),": Speak to robots like humans"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Hands-Free"}),": No need for keyboards or controllers"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Accessibility"}),": Easier for non-technical users"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multitasking"}),": Control robots while doing other tasks"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"web-speech-api-free-browser",children:"Web Speech API (Free, Browser)"}),"\n",(0,i.jsx)(e.p,{children:"The Web Speech API is completely free and works in any modern browser. No installation or API keys required!"}),"\n",(0,i.jsx)(e.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-mermaid",children:"graph LR\r\n    A[Microphone] --\x3e B[Browser]\r\n    B --\x3e C[Web Speech API]\r\n    C --\x3e D[Text Output]\r\n    D --\x3e E[Robot Control]\r\n    \r\n    style C fill:#c8e6c9\n"})}),"\n",(0,i.jsx)(e.h3,{id:"basic-implementation",children:"Basic Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-html",children:"<!DOCTYPE html>\r\n<html>\r\n<head>\r\n    <title>Voice Control Robot</title>\r\n</head>\r\n<body>\r\n    <h1>Voice Controlled Robot</h1>\r\n    <button id=\"startBtn\">Start Listening</button>\r\n    <button id=\"stopBtn\">Stop Listening</button>\r\n    <p id=\"status\">Ready</p>\r\n    <p id=\"transcript\">Transcript will appear here...</p>\r\n\r\n    <script>\r\n        const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\r\n        \r\n        recognition.continuous = true;\r\n        recognition.interimResults = true;\r\n        recognition.lang = 'en-US';\r\n\r\n        const startBtn = document.getElementById('startBtn');\r\n        const stopBtn = document.getElementById('stopBtn');\r\n        const status = document.getElementById('status');\r\n        const transcript = document.getElementById('transcript');\r\n\r\n        startBtn.addEventListener('click', () => {\r\n            recognition.start();\r\n            status.textContent = 'Listening...';\r\n        });\r\n\r\n        stopBtn.addEventListener('click', () => {\r\n            recognition.stop();\r\n            status.textContent = 'Stopped';\r\n        });\r\n\r\n        recognition.onresult = (event) => {\r\n            let interimTranscript = '';\r\n            let finalTranscript = '';\r\n\r\n            for (let i = event.resultIndex; i < event.results.length; i++) {\r\n                const transcript = event.results[i][0].transcript;\r\n                if (event.results[i].isFinal) {\r\n                    finalTranscript += transcript + ' ';\r\n                } else {\r\n                    interimTranscript += transcript;\r\n                }\r\n            }\r\n\r\n            transcript.textContent = finalTranscript || interimTranscript;\r\n            \r\n            // Process command when final\r\n            if (finalTranscript) {\r\n                processCommand(finalTranscript.trim());\r\n            }\r\n        };\r\n\r\n        recognition.onerror = (event) => {\r\n            console.error('Speech recognition error:', event.error);\r\n            status.textContent = 'Error: ' + event.error;\r\n        };\r\n\r\n        function processCommand(command) {\r\n            console.log('Command received:', command);\r\n            // Send to robot via WebSocket or HTTP\r\n            sendToRobot(command);\r\n        }\r\n\r\n        function sendToRobot(command) {\r\n            // Example: Send via WebSocket\r\n            const ws = new WebSocket('ws://localhost:8080');\r\n            ws.onopen = () => {\r\n                ws.send(JSON.stringify({command: command}));\r\n            };\r\n        }\r\n    <\/script>\r\n</body>\r\n</html>\n"})}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-bridge-for-web-speech",children:"ROS 2 Bridge for Web Speech"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nWeb Speech API to ROS 2 Bridge\r\n\r\nBridge between browser-based speech recognition and ROS 2.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom flask import Flask, request, jsonify\r\nfrom flask_cors import CORS\r\nimport threading\r\n\r\napp = Flask(__name__)\r\nCORS(app)  # Allow browser connections\r\n\r\nclass WebSpeechBridge(Node):\r\n    def __init__(self):\r\n        super().__init__('web_speech_bridge')\r\n        \r\n        # Publisher for voice commands\r\n        self.command_pub = self.create_publisher(\r\n            String,\r\n            '/voice_command',\r\n            10\r\n        )\r\n        \r\n        self.get_logger().info('Web Speech bridge started')\r\n\r\n@app.route('/command', methods=['POST'])\r\ndef receive_command():\r\n    \"\"\"Receive command from web browser.\"\"\"\r\n    data = request.json\r\n    command = data.get('command', '')\r\n    \r\n    # Publish to ROS 2\r\n    bridge.command_pub.publish(String(data=command))\r\n    \r\n    return jsonify({'status': 'received', 'command': command})\r\n\r\ndef main(args=None):\r\n    global bridge\r\n    \r\n    rclpy.init(args=args)\r\n    bridge = WebSpeechBridge()\r\n    \r\n    # Run Flask in separate thread\r\n    flask_thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=8080))\r\n    flask_thread.daemon = True\r\n    flask_thread.start()\r\n    \r\n    try:\r\n        rclpy.spin(bridge)\r\n    except KeyboardInterrupt:\r\n        bridge.get_logger().info('Shutting down...')\r\n    finally:\r\n        bridge.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"openai-whisper-api",children:"OpenAI Whisper API"}),"\n",(0,i.jsx)(e.p,{children:"OpenAI Whisper provides high-accuracy speech recognition. It has a free tier and is very affordable."}),"\n",(0,i.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Install OpenAI library\r\npip install openai\r\n\r\n# Or use whisper directly (local, free)\r\npip install open-whisper\n"})}),"\n",(0,i.jsx)(e.h3,{id:"using-whisper-api",children:"Using Whisper API"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nOpenAI Whisper Speech Recognition\r\n\r\nHigh-accuracy speech recognition using Whisper API.\r\n"""\r\n\r\nimport openai\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport pyaudio\r\nimport wave\r\nimport io\r\n\r\nclass WhisperSpeechRecognition(Node):\r\n    def __init__(self):\r\n        super().__init__(\'whisper_speech_recognition\')\r\n        \r\n        # Set OpenAI API key\r\n        openai.api_key = "your-api-key-here"  # Get from OpenAI\r\n        \r\n        # Publisher\r\n        self.command_pub = self.create_publisher(\r\n            String,\r\n            \'/voice_command\',\r\n            10\r\n        )\r\n        \r\n        # Audio setup\r\n        self.audio = pyaudio.PyAudio()\r\n        self.stream = None\r\n        \r\n        self.get_logger().info(\'Whisper speech recognition started\')\r\n    \r\n    def record_audio(self, duration=5):\r\n        """Record audio from microphone."""\r\n        chunk = 1024\r\n        sample_format = pyaudio.paInt16\r\n        channels = 1\r\n        fs = 44100\r\n        \r\n        self.get_logger().info(\'Recording...\')\r\n        \r\n        self.stream = self.audio.open(\r\n            format=sample_format,\r\n            channels=channels,\r\n            rate=fs,\r\n            frames_per_buffer=chunk,\r\n            input=True\r\n        )\r\n        \r\n        frames = []\r\n        for _ in range(0, int(fs / chunk * duration)):\r\n            data = self.stream.read(chunk)\r\n            frames.append(data)\r\n        \r\n        self.stream.stop_stream()\r\n        self.stream.close()\r\n        \r\n        # Save to WAV\r\n        wf = wave.open(\'temp_audio.wav\', \'wb\')\r\n        wf.setnchannels(channels)\r\n        wf.setsampwidth(self.audio.get_sample_size(sample_format))\r\n        wf.setframerate(fs)\r\n        wf.writeframes(b\'\'.join(frames))\r\n        wf.close()\r\n        \r\n        return \'temp_audio.wav\'\r\n    \r\n    def transcribe_audio(self, audio_file):\r\n        """Transcribe audio using Whisper API."""\r\n        try:\r\n            with open(audio_file, \'rb\') as f:\r\n                transcript = openai.Audio.transcribe(\r\n                    model="whisper-1",\r\n                    file=f,\r\n                    language="en"\r\n                )\r\n            return transcript[\'text\']\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Transcription error: {e}\')\r\n            return None\r\n    \r\n    def process_voice_command(self):\r\n        """Main voice command processing loop."""\r\n        while rclpy.ok():\r\n            # Record audio\r\n            audio_file = self.record_audio(duration=3)\r\n            \r\n            # Transcribe\r\n            text = self.transcribe_audio(audio_file)\r\n            \r\n            if text:\r\n                self.get_logger().info(f\'Recognized: {text}\')\r\n                \r\n                # Publish command\r\n                msg = String()\r\n                msg.data = text\r\n                self.command_pub.publish(msg)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = WhisperSpeechRecognition()\r\n    \r\n    # Run in separate thread\r\n    import threading\r\n    voice_thread = threading.Thread(target=node.process_voice_command)\r\n    voice_thread.start()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.audio.terminate()\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"local-whisper-completely-free",children:"Local Whisper (Completely Free)"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nLocal Whisper (Free, No API)\r\n\r\nRun Whisper locally without API calls.\r\n"""\r\n\r\nimport whisper\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport pyaudio\r\nimport wave\r\n\r\nclass LocalWhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'local_whisper_node\')\r\n        \r\n        # Load Whisper model (downloads automatically, free)\r\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\r\n        \r\n        # Publisher\r\n        self.command_pub = self.create_publisher(String, \'/voice_command\', 10)\r\n        \r\n        self.get_logger().info(\'Local Whisper node started\')\r\n    \r\n    def record_and_transcribe(self):\r\n        """Record audio and transcribe."""\r\n        # Record audio (same as before)\r\n        audio_file = self.record_audio()\r\n        \r\n        # Transcribe locally (free, no API)\r\n        result = self.model.transcribe(audio_file, language="en")\r\n        text = result["text"]\r\n        \r\n        self.get_logger().info(f\'Transcribed: {text}\')\r\n        \r\n        # Publish\r\n        msg = String()\r\n        msg.data = text\r\n        self.command_pub.publish(msg)\r\n        \r\n        return text\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LocalWhisperNode()\r\n    \r\n    # Process commands\r\n    while rclpy.ok():\r\n        node.record_and_transcribe()\r\n        rclpy.spin_once(node, timeout_sec=0.1)\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"real-time-recognition",children:"Real-Time Recognition"}),"\n",(0,i.jsx)(e.h3,{id:"continuous-listening",children:"Continuous Listening"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nReal-Time Voice Recognition\r\n\r\nContinuous speech recognition with low latency.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport speech_recognition as sr\r\nimport threading\r\n\r\nclass RealTimeVoiceRecognition(Node):\r\n    def __init__(self):\r\n        super().__init__('realtime_voice_recognition')\r\n        \r\n        # Use speech_recognition library (free)\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n        \r\n        # Publisher\r\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\r\n        \r\n        # Start listening thread\r\n        self.listening = True\r\n        self.listen_thread = threading.Thread(target=self.listen_continuously)\r\n        self.listen_thread.start()\r\n        \r\n        self.get_logger().info('Real-time voice recognition started')\r\n    \r\n    def listen_continuously(self):\r\n        \"\"\"Continuously listen for speech.\"\"\"\r\n        while self.listening and rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    # Listen with timeout\r\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\r\n                \r\n                # Recognize using Google (free, requires internet)\r\n                try:\r\n                    text = self.recognizer.recognize_google(audio)\r\n                    self.get_logger().info(f'Recognized: {text}')\r\n                    \r\n                    # Publish command\r\n                    msg = String()\r\n                    msg.data = text\r\n                    self.command_pub.publish(msg)\r\n                \r\n                except sr.UnknownValueError:\r\n                    # Could not understand audio\r\n                    pass\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f'Recognition error: {e}')\r\n            \r\n            except sr.WaitTimeoutError:\r\n                # No speech detected, continue\r\n                pass\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = RealTimeVoiceRecognition()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.listening = False\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"complete-voice-system",children:"Complete Voice System"}),"\n",(0,i.jsx)(e.h3,{id:"full-integration",children:"Full Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nComplete Voice Command System\r\n\r\nIntegrates speech recognition with robot control.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport speech_recognition as sr\r\nimport threading\r\n\r\nclass VoiceCommandSystem(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_command_system')\r\n        \r\n        # Speech recognition\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n        \r\n        # Publishers\r\n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        \r\n        # Start listening\r\n        self.listening = True\r\n        self.listen_thread = threading.Thread(target=self.listen_loop)\r\n        self.listen_thread.start()\r\n        \r\n        self.get_logger().info('Voice command system started')\r\n    \r\n    def listen_loop(self):\r\n        \"\"\"Main listening loop.\"\"\"\r\n        while self.listening and rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\r\n                \r\n                try:\r\n                    text = self.recognizer.recognize_google(audio).lower()\r\n                    self.get_logger().info(f'Command: {text}')\r\n                    \r\n                    # Process command\r\n                    self.process_command(text)\r\n                \r\n                except sr.UnknownValueError:\r\n                    pass\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f'Recognition error: {e}')\r\n            \r\n            except sr.WaitTimeoutError:\r\n                pass\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error: {e}')\r\n    \r\n    def process_command(self, command):\r\n        \"\"\"Process voice command and control robot.\"\"\"\r\n        # Publish raw command\r\n        msg = String()\r\n        msg.data = command\r\n        self.command_pub.publish(msg)\r\n        \r\n        # Simple command parsing\r\n        cmd = Twist()\r\n        \r\n        if 'forward' in command or 'ahead' in command:\r\n            cmd.linear.x = 0.5\r\n            self.get_logger().info('Moving forward')\r\n        elif 'backward' in command or 'back' in command:\r\n            cmd.linear.x = -0.5\r\n            self.get_logger().info('Moving backward')\r\n        elif 'left' in command:\r\n            cmd.angular.z = 0.5\r\n            self.get_logger().info('Turning left')\r\n        elif 'right' in command:\r\n            cmd.angular.z = -0.5\r\n            self.get_logger().info('Turning right')\r\n        elif 'stop' in command or 'halt' in command:\r\n            cmd.linear.x = 0.0\r\n            cmd.angular.z = 0.0\r\n            self.get_logger().info('Stopping')\r\n        else:\r\n            # Unknown command, will be processed by LLM later\r\n            return\r\n        \r\n        # Publish velocity command\r\n        self.cmd_vel_pub.publish(cmd)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceCommandSystem()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.listening = False\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,i.jsx)(e.h3,{id:"comparison",children:"Comparison"}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Method"}),(0,i.jsx)(e.th,{children:"Accuracy"}),(0,i.jsx)(e.th,{children:"Latency"}),(0,i.jsx)(e.th,{children:"Cost"}),(0,i.jsx)(e.th,{children:"Offline"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Web Speech API"}),(0,i.jsx)(e.td,{children:"85%"}),(0,i.jsx)(e.td,{children:"200ms"}),(0,i.jsx)(e.td,{children:"Free"}),(0,i.jsx)(e.td,{children:"No"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Google Speech"}),(0,i.jsx)(e.td,{children:"90%"}),(0,i.jsx)(e.td,{children:"300ms"}),(0,i.jsx)(e.td,{children:"Free"}),(0,i.jsx)(e.td,{children:"No"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Whisper API"}),(0,i.jsx)(e.td,{children:"95%"}),(0,i.jsx)(e.td,{children:"500ms"}),(0,i.jsx)(e.td,{children:"$0.006/min"}),(0,i.jsx)(e.td,{children:"No"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:"Local Whisper"}),(0,i.jsx)(e.td,{children:"95%"}),(0,i.jsx)(e.td,{children:"2000ms"}),(0,i.jsx)(e.td,{children:"Free"}),(0,i.jsx)(e.td,{children:"Yes"})]})]})]}),"\n",(0,i.jsx)(e.h3,{id:"recommendations",children:"Recommendations"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Learning/Prototyping"}),": Web Speech API (free, easy)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Production (Online)"}),": Whisper API (accurate, cheap)"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Production (Offline)"}),": Local Whisper (free, accurate)"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Noise Reduction"}),": Use good microphone, reduce ambient noise"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Clear Speech"}),": Speak clearly and at moderate pace"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Command Format"}),": Use consistent command phrases"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error Handling"}),": Handle recognition failures gracefully"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feedback"}),": Provide audio/visual feedback"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"common-errors-and-solutions",children:"Common Errors and Solutions"}),"\n",(0,i.jsx)(e.h3,{id:"error-1-microphone-not-found",children:'Error 1: "Microphone not found"'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Solution: List available microphones\r\nimport speech_recognition as sr\r\nfor index, name in enumerate(sr.Microphone.list_microphone_names()):\r\n    print(f"Microphone {index}: {name}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"error-2-recognition-timeout",children:'Error 2: "Recognition timeout"'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Solution: Increase timeout\r\naudio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"error-3-api-key-not-set",children:'Error 3: "API key not set"'}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Solution: Set API key\r\nimport os\r\nos.environ['OPENAI_API_KEY'] = 'your-key-here'\n"})}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(e.p,{children:"Continue learning:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/module-04-vla/llm-integration",children:"Chapter 3: LLM Integration"})," - Add intelligence with $5 OpenAI credit"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.a,{href:"/physical-ai-textbook/docs/module-04-vla/action-planning",children:"Chapter 4: Action Planning"})," - Plan complex tasks"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);