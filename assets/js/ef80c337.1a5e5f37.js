"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[710],{7491:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-04-vla/multimodal","title":"Chapter 5: Multimodal Systems","description":"Introduction","source":"@site/docs/module-04-vla/05-multimodal.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/multimodal","permalink":"/physical-ai-textbook/docs/module-04-vla/multimodal","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/05-multimodal.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Action Planning","permalink":"/physical-ai-textbook/docs/module-04-vla/action-planning"},"next":{"title":"Chapter 6: Capstone Project - Autonomous Butler","permalink":"/physical-ai-textbook/docs/module-04-vla/capstone-project"}}');var o=r(4848),s=r(8453);const i={sidebar_position:5},a="Chapter 5: Multimodal Systems",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Multimodal Architecture",id:"multimodal-architecture",level:3},{value:"Vision + Language + Action",id:"vision--language--action",level:2},{value:"Complete Pipeline",id:"complete-pipeline",level:3},{value:"Real-Time Processing",id:"real-time-processing",level:2},{value:"Optimized Pipeline",id:"optimized-pipeline",level:3},{value:"Latency Optimization",id:"latency-optimization",level:2},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Example: Voice Navigation",id:"example-voice-navigation",level:2},{value:"Complete Voice Navigation System",id:"complete-voice-navigation-system",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Latency Measurements",id:"latency-measurements",level:3},{value:"Throughput",id:"throughput",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-5-multimodal-systems",children:"Chapter 5: Multimodal Systems"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Multimodal systems combine vision, language, and action into a unified pipeline. This chapter covers building complete VLA systems that process multiple modalities simultaneously for robust robot control."}),"\n",(0,o.jsx)(n.h3,{id:"multimodal-architecture",children:"Multimodal Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TB\r\n    A[Voice Input] --\x3e B[Speech Recognition]\r\n    C[Camera] --\x3e D[Vision Processing]\r\n    \r\n    B --\x3e E[Language Understanding]\r\n    D --\x3e F[Scene Understanding]\r\n    \r\n    E --\x3e G[Multimodal Fusion]\r\n    F --\x3e G\r\n    \r\n    G --\x3e H[Action Planning]\r\n    H --\x3e I[Action Execution]\r\n    I --\x3e J[Robot Control]\r\n    \r\n    style G fill:#c8e6c9\r\n    style H fill:#fff9c4\r\n    style J fill:#ffccbc\n"})}),"\n",(0,o.jsx)(n.h2,{id:"vision--language--action",children:"Vision + Language + Action"}),"\n",(0,o.jsx)(n.h3,{id:"complete-pipeline",children:"Complete Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nComplete Multimodal VLA System\r\n\r\nIntegrates vision, language, and action.\r\n"""\r\n\r\nimport openai\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Twist\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport json\r\nimport base64\r\n\r\nclass MultimodalVLA(Node):\r\n    def __init__(self):\r\n        super().__init__(\'multimodal_vla\')\r\n        \r\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n        \r\n        # Components\r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        \r\n        # State\r\n        self.current_image = None\r\n        self.detected_objects = []\r\n        self.robot_pose = None\r\n        \r\n        # Subscribers\r\n        self.voice_sub = self.create_subscription(\r\n            String, \'/voice_command\', self.voice_callback, 10\r\n        )\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.action_pub = self.create_publisher(String, \'/robot_action\', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        self.get_logger().info(\'Multimodal VLA system started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Process images for vision."""\r\n        try:\r\n            self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            \r\n            # Detect objects\r\n            results = self.yolo_model(self.current_image)\r\n            self.detected_objects = []\r\n            \r\n            for result in results:\r\n                for box in result.boxes:\r\n                    if float(box.conf[0]) > 0.5:\r\n                        self.detected_objects.append({\r\n                            \'label\': self.yolo_model.names[int(box.cls[0])],\r\n                            \'confidence\': float(box.conf[0]),\r\n                            \'bbox\': box.xyxy[0].cpu().numpy().tolist()\r\n                        })\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Vision error: {e}\')\r\n    \r\n    def voice_callback(self, msg):\r\n        """Process voice commands with vision context."""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Processing: {command}\')\r\n        \r\n        # Create multimodal prompt\r\n        prompt = self.create_multimodal_prompt(command)\r\n        \r\n        # Get action from LLM\r\n        action = self.get_multimodal_action(prompt)\r\n        \r\n        if action:\r\n            self.execute_action(action)\r\n    \r\n    def create_multimodal_prompt(self, command):\r\n        """Create prompt with vision and language."""\r\n        # Build scene description\r\n        objects_str = \', \'.join([\r\n            f"{obj[\'label\']} (confidence: {obj[\'confidence\']:.2f})"\r\n            for obj in self.detected_objects\r\n        ])\r\n        \r\n        scene_description = f"""Current scene contains: {objects_str if objects_str else "no objects detected"}\r\n\r\nRobot can see: {len(self.detected_objects)} objects"""\r\n        \r\n        prompt = f"""You are a robot assistant with vision capabilities.\r\n\r\nUser command: "{command}"\r\n\r\n{scene_description}\r\n\r\nBased on the command and what the robot can see, determine the action to take.\r\n\r\nAvailable actions:\r\n- move(direction, distance)\r\n- turn(direction, angle)\r\n- pick(object)\r\n- place(object, location)\r\n- navigate_to(location)\r\n- stop()\r\n\r\nRespond with JSON:\r\n{{\r\n    "action": "action_name",\r\n    "parameters": {{...}},\r\n    "reasoning": "Why this action based on vision and command",\r\n    "target_object": "object name if applicable"\r\n}}"""\r\n        \r\n        return prompt\r\n    \r\n    def get_multimodal_action(self, prompt):\r\n        """Get action from LLM with multimodal context."""\r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": "You are a multimodal robot controller. Use vision and language to determine actions."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                temperature=0.3,\r\n                max_tokens=300\r\n            )\r\n            \r\n            text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            if \'```\' in text:\r\n                text = text.split(\'```\')[1]\r\n                if text.startswith(\'json\'):\r\n                    text = text[4:]\r\n            \r\n            return json.loads(text)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Multimodal action error: {e}\')\r\n            return None\r\n    \r\n    def execute_action(self, action):\r\n        """Execute multimodal action."""\r\n        action_type = action.get(\'action\')\r\n        params = action.get(\'parameters\', {})\r\n        reasoning = action.get(\'reasoning\', \'\')\r\n        \r\n        self.get_logger().info(f\'Action: {action_type}, Reasoning: {reasoning}\')\r\n        \r\n        # Execute based on action type\r\n        # (Implementation similar to previous examples)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = MultimodalVLA()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-processing",children:"Real-Time Processing"}),"\n",(0,o.jsx)(n.h3,{id:"optimized-pipeline",children:"Optimized Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nOptimized Real-Time Multimodal System\r\n\r\nLow-latency processing for real-time control.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nimport threading\r\nimport queue\r\nimport time\r\n\r\nclass RealTimeMultimodal(Node):\r\n    def __init__(self):\r\n        super().__init__(\'realtime_multimodal\')\r\n        \r\n        # Processing queues\r\n        self.image_queue = queue.Queue(maxsize=1)\r\n        self.command_queue = queue.Queue(maxsize=1)\r\n        \r\n        # Processing threads\r\n        self.vision_thread = threading.Thread(target=self.vision_processor)\r\n        self.language_thread = threading.Thread(target=self.language_processor)\r\n        self.fusion_thread = threading.Thread(target=self.fusion_processor)\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 1  # Small queue\r\n        )\r\n        self.command_sub = self.create_subscription(\r\n            String, \'/voice_command\', self.command_callback, 1\r\n        )\r\n        \r\n        # Start threads\r\n        self.vision_thread.start()\r\n        self.language_thread.start()\r\n        self.fusion_thread.start()\r\n        \r\n        self.get_logger().info(\'Real-time multimodal system started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Queue image for processing."""\r\n        try:\r\n            self.image_queue.put_nowait(msg)\r\n        except queue.Full:\r\n            # Drop old frame, keep latest\r\n            try:\r\n                self.image_queue.get_nowait()\r\n                self.image_queue.put_nowait(msg)\r\n            except queue.Empty:\r\n                pass\r\n    \r\n    def command_callback(self, msg):\r\n        """Queue command for processing."""\r\n        try:\r\n            self.command_queue.put_nowait(msg)\r\n        except queue.Full:\r\n            pass\r\n    \r\n    def vision_processor(self):\r\n        """Process images in separate thread."""\r\n        while rclpy.ok():\r\n            try:\r\n                msg = self.image_queue.get(timeout=0.1)\r\n                # Process image\r\n                # (Vision processing code)\r\n                time.sleep(0.033)  # ~30 FPS\r\n            except queue.Empty:\r\n                continue\r\n    \r\n    def language_processor(self):\r\n        """Process commands in separate thread."""\r\n        while rclpy.ok():\r\n            try:\r\n                msg = self.command_queue.get(timeout=0.1)\r\n                # Process command\r\n                # (Language processing code)\r\n            except queue.Empty:\r\n                continue\r\n    \r\n    def fusion_processor(self):\r\n        """Fuse modalities in separate thread."""\r\n        while rclpy.ok():\r\n            # Combine vision and language results\r\n            # (Fusion code)\r\n            time.sleep(0.1)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = RealTimeMultimodal()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Reduce Image Resolution"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Downscale for faster processing\r\nsmall_image = cv2.resize(cv_image, (320, 240))\r\nresults = self.yolo_model(small_image)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Process Every Nth Frame"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"self.frame_count = 0\r\nif self.frame_count % 3 == 0:  # Process every 3rd frame\r\n    # Process image\r\n    pass\r\nself.frame_count += 1\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Use Smaller Models"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Fast model\r\nmodel = YOLO('yolov8n.pt')  # Nano (fastest)\r\n# Instead of\r\n# model = YOLO('yolov8x.pt')  # Extra large (slowest)\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Cache LLM Responses"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Cache common queries\r\ncache = {}\r\n\r\ndef get_cached_response(prompt):\r\n    if prompt in cache:\r\n        return cache[prompt]\r\n    # Call LLM and cache\r\n    response = call_llm(prompt)\r\n    cache[prompt] = response\r\n    return response\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"5",children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Parallel Processing"})}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import concurrent.futures\r\n\r\ndef process_multimodal(command, image):\r\n    with concurrent.futures.ThreadPoolExecutor() as executor:\r\n        vision_future = executor.submit(process_vision, image)\r\n        language_future = executor.submit(process_language, command)\r\n        \r\n        vision_result = vision_future.result()\r\n        language_result = language_future.result()\r\n        \r\n        return fuse(vision_result, language_result)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"example-voice-navigation",children:"Example: Voice Navigation"}),"\n",(0,o.jsx)(n.h3,{id:"complete-voice-navigation-system",children:"Complete Voice Navigation System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nVoice Navigation System\r\n\r\nNavigate using voice commands with vision feedback.\r\n"""\r\n\r\nimport openai\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport json\r\nimport math\r\n\r\nclass VoiceNavigationSystem(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_navigation\')\r\n        \r\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n        \r\n        # Components\r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO(\'yolov8n.pt\')\r\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\r\n        \r\n        # State\r\n        self.current_image = None\r\n        self.detected_landmarks = {}\r\n        \r\n        # Subscribers\r\n        self.voice_sub = self.create_subscription(\r\n            String, \'/voice_command\', self.voice_callback, 10\r\n        )\r\n        self.image_sub = self.create_subscription(\r\n            Image, \'/camera/image_raw\', self.image_callback, 10\r\n        )\r\n        \r\n        self.get_logger().info(\'Voice navigation system started\')\r\n    \r\n    def image_callback(self, msg):\r\n        """Process images to detect landmarks."""\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n            results = self.yolo_model(cv_image)\r\n            \r\n            # Update detected landmarks\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    label = self.yolo_model.names[int(box.cls[0])]\r\n                    if label in [\'door\', \'chair\', \'table\', \'person\']:\r\n                        self.detected_landmarks[label] = {\r\n                            \'confidence\': float(box.conf[0]),\r\n                            \'bbox\': box.xyxy[0].cpu().numpy().tolist()\r\n                        }\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Vision error: {e}\')\r\n    \r\n    def voice_callback(self, msg):\r\n        """Process voice navigation commands."""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Navigation command: {command}\')\r\n        \r\n        # Create navigation prompt\r\n        prompt = self.create_navigation_prompt(command)\r\n        \r\n        # Get navigation goal\r\n        goal = self.get_navigation_goal(prompt)\r\n        \r\n        if goal:\r\n            self.navigate_to_goal(goal)\r\n    \r\n    def create_navigation_prompt(self, command):\r\n        """Create prompt for navigation."""\r\n        landmarks = list(self.detected_landmarks.keys())\r\n        landmarks_str = \', \'.join(landmarks) if landmarks else \'none\'\r\n        \r\n        prompt = f"""User wants to navigate: "{command}"\r\n\r\nRobot can currently see: {landmarks_str}\r\n\r\nDetermine the navigation goal. If a specific location is mentioned, use that.\r\nIf an object is mentioned and visible, navigate towards it.\r\n\r\nRespond with JSON:\r\n{{\r\n    "goal_type": "location|object|relative",\r\n    "target": "location name or object name",\r\n    "x": 0.0,\r\n    "y": 0.0,\r\n    "theta": 0.0,\r\n    "reasoning": "Why this goal"\r\n}}"""\r\n        \r\n        return prompt\r\n    \r\n    def get_navigation_goal(self, prompt):\r\n        """Get navigation goal from LLM."""\r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": "You are a navigation planner. Return valid JSON."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                temperature=0.3,\r\n                max_tokens=200\r\n            )\r\n            \r\n            text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            if \'```\' in text:\r\n                text = text.split(\'```\')[1]\r\n                if text.startswith(\'json\'):\r\n                    text = text[4:]\r\n            \r\n            return json.loads(text)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Navigation planning error: {e}\')\r\n            return None\r\n    \r\n    def navigate_to_goal(self, goal):\r\n        """Navigate to goal position."""\r\n        goal_msg = NavigateToPose.Goal()\r\n        goal_msg.pose.header.frame_id = \'map\'\r\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\r\n        \r\n        goal_msg.pose.pose.position.x = goal.get(\'x\', 0.0)\r\n        goal_msg.pose.pose.position.y = goal.get(\'y\', 0.0)\r\n        goal_msg.pose.pose.orientation.z = math.sin(goal.get(\'theta\', 0.0) / 2.0)\r\n        goal_msg.pose.pose.orientation.w = math.cos(goal.get(\'theta\', 0.0) / 2.0)\r\n        \r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal_msg)\r\n        \r\n        rclpy.spin_until_future_complete(self, future)\r\n        goal_handle = future.result()\r\n        \r\n        if goal_handle.accepted:\r\n            self.get_logger().info(f\'Navigating to: {goal.get("target", "goal")}\')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceNavigationSystem()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,o.jsx)(n.h3,{id:"latency-measurements",children:"Latency Measurements"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Component"}),(0,o.jsx)(n.th,{children:"Processing Time"}),(0,o.jsx)(n.th,{children:"Optimization"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Vision (YOLO)"}),(0,o.jsx)(n.td,{children:"50-100ms"}),(0,o.jsx)(n.td,{children:"Use nano model, downscale"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Language (GPT)"}),(0,o.jsx)(n.td,{children:"200-500ms"}),(0,o.jsx)(n.td,{children:"Use GPT-3.5-turbo, cache"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Fusion"}),(0,o.jsx)(n.td,{children:"10-20ms"}),(0,o.jsx)(n.td,{children:"Parallel processing"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Total"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"260-620ms"})}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Optimized: 150-300ms"})})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"throughput",children:"Throughput"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Commands per minute"}),": 10-20 (unoptimized)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Commands per minute"}),": 30-50 (optimized)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cost per command"}),": ~$0.0003"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parallel Processing"}),": Process vision and language simultaneously"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Frame Skipping"}),": Don't process every frame"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Selection"}),": Use smallest model that meets accuracy needs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Caching"}),": Cache common LLM queries"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Handling"}),": Robust error handling for real-time systems"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"Continue learning:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/physical-ai-textbook/docs/module-04-vla/capstone-project",children:"Chapter 6: Capstone Project"})," - Build complete autonomous butler"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(6540);const o={},s=t.createContext(o);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);