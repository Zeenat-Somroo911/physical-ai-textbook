"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[8740],{8312:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"module-04-vla/capstone-project","title":"Chapter 6: Capstone Project - Autonomous Butler","description":"Project Overview","source":"@site/docs/module-04-vla/06-capstone-project.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/capstone-project","permalink":"/physical-ai-textbook/docs/module-04-vla/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/06-capstone-project.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Multimodal Systems","permalink":"/physical-ai-textbook/docs/module-04-vla/multimodal"},"next":{"title":"Projects Overview","permalink":"/physical-ai-textbook/docs/projects/overview"}}');var t=r(4848),o=r(8453);const i={sidebar_position:6},l="Chapter 6: Capstone Project - Autonomous Butler",c={},a=[{value:"Project Overview",id:"project-overview",level:2},{value:"Project Goals",id:"project-goals",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"Project Structure",id:"project-structure",level:2},{value:"Directory Layout",id:"directory-layout",level:3},{value:"Step-by-Step Implementation",id:"step-by-step-implementation",level:2},{value:"Step 1: Create ROS 2 Package",id:"step-1-create-ros-2-package",level:3},{value:"Step 2: Voice Recognizer Node",id:"step-2-voice-recognizer-node",level:3},{value:"Step 3: Vision Processor Node",id:"step-3-vision-processor-node",level:3},{value:"Step 4: Task Planner Node",id:"step-4-task-planner-node",level:3},{value:"Step 5: Action Executor Node",id:"step-5-action-executor-node",level:3},{value:"Step 6: Butler Controller (Main Node)",id:"step-6-butler-controller-main-node",level:3},{value:"Step 7: Launch File",id:"step-7-launch-file",level:3},{value:"Step 8: Update setup.py",id:"step-8-update-setuppy",level:3},{value:"Testing and Debugging",id:"testing-and-debugging",level:2},{value:"Test Scenarios",id:"test-scenarios",level:3},{value:"Scenario 1: Simple Navigation",id:"scenario-1-simple-navigation",level:4},{value:"Scenario 2: Object Manipulation",id:"scenario-2-object-manipulation",level:4},{value:"Scenario 3: Complex Task",id:"scenario-3-complex-task",level:4},{value:"Debugging Checklist",id:"debugging-checklist",level:3},{value:"Common Issues",id:"common-issues",level:3},{value:"Issue 1: Voice not recognized",id:"issue-1-voice-not-recognized",level:4},{value:"Issue 2: Objects not detected",id:"issue-2-objects-not-detected",level:4},{value:"Issue 3: Plans not generated",id:"issue-3-plans-not-generated",level:4},{value:"Issue 4: Actions not executing",id:"issue-4-actions-not-executing",level:4},{value:"Demo Scenarios",id:"demo-scenarios",level:2},{value:"Demo 1: Basic Commands",id:"demo-1-basic-commands",level:3},{value:"Demo 2: Object Interaction",id:"demo-2-object-interaction",level:3},{value:"Demo 3: Complex Task",id:"demo-3-complex-task",level:3},{value:"Demo 4: Multi-Step Task",id:"demo-4-multi-step-task",level:3},{value:"Grading Rubric",id:"grading-rubric",level:2},{value:"Functionality (40 points)",id:"functionality-40-points",level:3},{value:"Code Quality (20 points)",id:"code-quality-20-points",level:3},{value:"Integration (20 points)",id:"integration-20-points",level:3},{value:"Performance (10 points)",id:"performance-10-points",level:3},{value:"Documentation (10 points)",id:"documentation-10-points",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Expected Performance",id:"expected-performance",level:3},{value:"Cost Analysis",id:"cost-analysis",level:3},{value:"Deployment Tips",id:"deployment-tips",level:2},{value:"1. Production Considerations",id:"1-production-considerations",level:3},{value:"2. Optimization",id:"2-optimization",level:3},{value:"3. Scaling",id:"3-scaling",level:3},{value:"Module 4 Summary",id:"module-4-summary",level:2},{value:"Course Summary",id:"course-summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-6-capstone-project---autonomous-butler",children:"Chapter 6: Capstone Project - Autonomous Butler"})}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsxs)(n.p,{children:["This capstone project integrates all concepts from the course to build an ",(0,t.jsx)(n.strong,{children:"Autonomous Butler"})," - a robot that can understand natural language commands, perceive its environment, and execute complex tasks autonomously."]}),"\n",(0,t.jsx)(n.h3,{id:"project-goals",children:"Project Goals"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Control"}),": Respond to voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Perception"}),": Understand the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Planning"}),": Break down complex commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Autonomous Execution"}),": Complete tasks independently"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost"}),": Under $5 total (using free tools)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TB\r\n    A[User Voice Command] --\x3e B[Speech Recognition]\r\n    B --\x3e C[Language Understanding]\r\n    D[Camera] --\x3e E[Object Detection]\r\n    E --\x3e F[Scene Understanding]\r\n    \r\n    C --\x3e G[Multimodal Fusion]\r\n    F --\x3e G\r\n    \r\n    G --\x3e H[Task Planner]\r\n    H --\x3e I[Action Executor]\r\n    I --\x3e J[Robot Control]\r\n    \r\n    J --\x3e K[Feedback]\r\n    K --\x3e L[Status Updates]\r\n    \r\n    style A fill:#e1f5ff\r\n    style G fill:#c8e6c9\r\n    style I fill:#fff9c4\r\n    style J fill:#ffccbc\n"})}),"\n",(0,t.jsx)(n.h2,{id:"project-structure",children:"Project Structure"}),"\n",(0,t.jsx)(n.h3,{id:"directory-layout",children:"Directory Layout"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"autonomous_butler/\r\n\u251c\u2500\u2500 package.xml\r\n\u251c\u2500\u2500 setup.py\r\n\u251c\u2500\u2500 autonomous_butler/\r\n\u2502   \u251c\u2500\u2500 __init__.py\r\n\u2502   \u251c\u2500\u2500 nodes/\r\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\r\n\u2502   \u2502   \u251c\u2500\u2500 voice_recognizer.py\r\n\u2502   \u2502   \u251c\u2500\u2500 vision_processor.py\r\n\u2502   \u2502   \u251c\u2500\u2500 task_planner.py\r\n\u2502   \u2502   \u251c\u2500\u2500 action_executor.py\r\n\u2502   \u2502   \u2514\u2500\u2500 butler_controller.py\r\n\u2502   \u2514\u2500\u2500 launch/\r\n\u2502       \u2514\u2500\u2500 butler.launch.py\r\n\u251c\u2500\u2500 config/\r\n\u2502   \u251c\u2500\u2500 butler_config.yaml\r\n\u2502   \u2514\u2500\u2500 action_primitives.yaml\r\n\u2514\u2500\u2500 README.md\n"})}),"\n",(0,t.jsx)(n.h2,{id:"step-by-step-implementation",children:"Step-by-Step Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-create-ros-2-package",children:"Step 1: Create ROS 2 Package"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\r\nros2 pkg create --build-type ament_python autonomous_butler \\\r\n    --dependencies rclpy std_msgs sensor_msgs geometry_msgs nav_msgs\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-voice-recognizer-node",children:"Step 2: Voice Recognizer Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/nodes/voice_recognizer.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVoice Recognizer Node\r\n\r\nFree speech recognition for butler commands.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport speech_recognition as sr\r\nimport threading\r\n\r\nclass VoiceRecognizer(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_recognizer')\r\n        \r\n        # Speech recognition\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n        \r\n        # Publisher\r\n        self.command_pub = self.create_publisher(String, '/butler/command', 10)\r\n        \r\n        # Start listening\r\n        self.listening = True\r\n        self.listen_thread = threading.Thread(target=self.listen_loop)\r\n        self.listen_thread.start()\r\n        \r\n        self.get_logger().info('Voice recognizer started')\r\n    \r\n    def listen_loop(self):\r\n        \"\"\"Continuous listening loop.\"\"\"\r\n        while self.listening and rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    audio = self.recognizer.listen(\r\n                        source,\r\n                        timeout=1,\r\n                        phrase_time_limit=5\r\n                    )\r\n                \r\n                try:\r\n                    text = self.recognizer.recognize_google(audio).lower()\r\n                    self.get_logger().info(f'Recognized: {text}')\r\n                    \r\n                    # Publish command\r\n                    msg = String()\r\n                    msg.data = text\r\n                    self.command_pub.publish(msg)\r\n                \r\n                except sr.UnknownValueError:\r\n                    pass\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f'Recognition error: {e}')\r\n            \r\n            except sr.WaitTimeoutError:\r\n                pass\r\n            except Exception as e:\r\n                self.get_logger().error(f'Error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceRecognizer()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.listening = False\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-vision-processor-node",children:"Step 3: Vision Processor Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/nodes/vision_processor.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nVision Processor Node\r\n\r\nObject detection and scene understanding.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nfrom ultralytics import YOLO\r\nimport json\r\n\r\nclass VisionProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__('vision_processor')\r\n        \r\n        self.bridge = CvBridge()\r\n        self.yolo_model = YOLO('yolov8n.pt')\r\n        \r\n        # Subscriber\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher\r\n        self.scene_pub = self.create_publisher(String, '/butler/scene', 10)\r\n        \r\n        # State\r\n        self.detected_objects = []\r\n        \r\n        self.get_logger().info('Vision processor started')\r\n    \r\n    def image_callback(self, msg):\r\n        \"\"\"Process images.\"\"\"\r\n        try:\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\r\n            \r\n            # Downscale for speed\r\n            small_image = cv2.resize(cv_image, (320, 240))\r\n            \r\n            # Detect objects\r\n            results = self.yolo_model(small_image)\r\n            \r\n            self.detected_objects = []\r\n            for result in results:\r\n                for box in result.boxes:\r\n                    if float(box.conf[0]) > 0.5:\r\n                        self.detected_objects.append({\r\n                            'label': self.yolo_model.names[int(box.cls[0])],\r\n                            'confidence': float(box.conf[0]),\r\n                            'bbox': box.xyxy[0].cpu().numpy().tolist()\r\n                        })\r\n            \r\n            # Publish scene description\r\n            scene_msg = String()\r\n            scene_msg.data = json.dumps(self.detected_objects)\r\n            self.scene_pub.publish(scene_msg)\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f'Vision error: {e}')\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionProcessor()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-task-planner-node",children:"Step 4: Task Planner Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/nodes/task_planner.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""\r\nTask Planner Node\r\n\r\nPlans tasks using LLM with vision context.\r\n"""\r\n\r\nimport openai\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport json\r\nimport os\r\n\r\nclass TaskPlanner(Node):\r\n    def __init__(self):\r\n        super().__init__(\'task_planner\')\r\n        \r\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n        \r\n        # Subscribers\r\n        self.command_sub = self.create_subscription(\r\n            String, \'/butler/command\', self.command_callback, 10\r\n        )\r\n        self.scene_sub = self.create_subscription(\r\n            String, \'/butler/scene\', self.scene_callback, 10\r\n        )\r\n        \r\n        # Publisher\r\n        self.plan_pub = self.create_publisher(String, \'/butler/plan\', 10)\r\n        \r\n        # State\r\n        self.current_scene = []\r\n        \r\n        self.get_logger().info(\'Task planner started\')\r\n    \r\n    def scene_callback(self, msg):\r\n        """Update scene information."""\r\n        try:\r\n            self.current_scene = json.loads(msg.data)\r\n        except:\r\n            self.current_scene = []\r\n    \r\n    def command_callback(self, msg):\r\n        """Plan task from command."""\r\n        command = msg.data\r\n        self.get_logger().info(f\'Planning: {command}\')\r\n        \r\n        # Generate plan\r\n        plan = self.generate_plan(command)\r\n        \r\n        if plan:\r\n            # Publish plan\r\n            plan_msg = String()\r\n            plan_msg.data = json.dumps(plan)\r\n            self.plan_pub.publish(plan_msg)\r\n    \r\n    def generate_plan(self, command):\r\n        """Generate plan using LLM."""\r\n        # Build scene description\r\n        objects = [obj[\'label\'] for obj in self.current_scene]\r\n        scene_desc = \', \'.join(objects) if objects else \'no objects\'\r\n        \r\n        prompt = f"""You are an autonomous butler robot. Plan the steps to complete this task.\r\n\r\nUser command: "{command}"\r\n\r\nCurrent scene: {scene_desc}\r\n\r\nAvailable actions:\r\n- navigate_to(location)\r\n- pick_up(object)\r\n- place(object, location)\r\n- open(door)\r\n- close(door)\r\n- check(object)\r\n- wait(duration)\r\n- stop()\r\n\r\nCreate a step-by-step plan. Return JSON array:\r\n[\r\n    {{"step": 1, "action": "action_name", "parameters": {{...}}, "description": "..."}},\r\n    ...\r\n]"""\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": "You are a butler robot planner. Return valid JSON arrays."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                temperature=0.3,\r\n                max_tokens=500\r\n            )\r\n            \r\n            text = response.choices[0].message.content.strip()\r\n            \r\n            # Extract JSON\r\n            if \'```\' in text:\r\n                text = text.split(\'```\')[1]\r\n                if text.startswith(\'json\'):\r\n                    text = text[4:]\r\n            \r\n            plan = json.loads(text)\r\n            self.get_logger().info(f\'Plan generated: {len(plan)} steps\')\r\n            return plan\r\n        \r\n        except Exception as e:\r\n            self.get_logger().error(f\'Planning error: {e}\')\r\n            return None\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = TaskPlanner()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Shutting down...\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-action-executor-node",children:"Step 5: Action Executor Node"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/nodes/action_executor.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nAction Executor Node\r\n\r\nExecutes planned actions.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom nav2_msgs.action import NavigateToPose\r\nfrom rclpy.action import ActionClient\r\nimport json\r\nimport math\r\n\r\nclass ActionExecutor(Node):\r\n    def __init__(self):\r\n        super().__init__('action_executor')\r\n        \r\n        # Subscriber\r\n        self.plan_sub = self.create_subscription(\r\n            String, '/butler/plan', self.plan_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.status_pub = self.create_publisher(String, '/butler/status', 10)\r\n        \r\n        # Action client\r\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\r\n        \r\n        # State\r\n        self.current_plan = []\r\n        self.current_step = 0\r\n        self.executing = False\r\n        \r\n        # Timer\r\n        self.timer = self.create_timer(0.1, self.execution_loop)\r\n        \r\n        self.get_logger().info('Action executor started')\r\n    \r\n    def plan_callback(self, msg):\r\n        \"\"\"Receive new plan.\"\"\"\r\n        try:\r\n            plan = json.loads(msg.data)\r\n            self.current_plan = plan\r\n            self.current_step = 0\r\n            self.executing = True\r\n            self.get_logger().info(f'New plan received: {len(plan)} steps')\r\n        except Exception as e:\r\n            self.get_logger().error(f'Plan parsing error: {e}')\r\n    \r\n    def execution_loop(self):\r\n        \"\"\"Main execution loop.\"\"\"\r\n        if self.executing and self.current_step < len(self.current_plan):\r\n            action = self.current_plan[self.current_step]\r\n            self.execute_action(action)\r\n    \r\n    def execute_action(self, action):\r\n        \"\"\"Execute a single action.\"\"\"\r\n        action_name = action.get('action')\r\n        params = action.get('parameters', {})\r\n        description = action.get('description', '')\r\n        \r\n        self.get_logger().info(f'Step {self.current_step + 1}: {description}')\r\n        \r\n        success = False\r\n        \r\n        if action_name == 'navigate_to':\r\n            success = self.navigate_to(params)\r\n        elif action_name == 'pick_up':\r\n            success = self.pick_up(params)\r\n        elif action_name == 'place':\r\n            success = self.place(params)\r\n        elif action_name == 'stop':\r\n            success = self.stop()\r\n        else:\r\n            self.get_logger().warn(f'Unknown action: {action_name}')\r\n        \r\n        if success:\r\n            self.current_step += 1\r\n            \r\n            if self.current_step >= len(self.current_plan):\r\n                self.executing = False\r\n                self.get_logger().info('Plan completed!')\r\n                \r\n                # Publish status\r\n                status_msg = String()\r\n                status_msg.data = 'completed'\r\n                self.status_pub.publish(status_msg)\r\n    \r\n    def navigate_to(self, params):\r\n        \"\"\"Navigate to location.\"\"\"\r\n        x = params.get('x', 0.0)\r\n        y = params.get('y', 0.0)\r\n        theta = params.get('theta', 0.0)\r\n        \r\n        goal = NavigateToPose.Goal()\r\n        goal.pose.header.frame_id = 'map'\r\n        goal.pose.header.stamp = self.get_clock().now().to_msg()\r\n        goal.pose.pose.position.x = x\r\n        goal.pose.pose.position.y = y\r\n        goal.pose.pose.orientation.z = math.sin(theta / 2.0)\r\n        goal.pose.pose.orientation.w = math.cos(theta / 2.0)\r\n        \r\n        self.nav_client.wait_for_server()\r\n        future = self.nav_client.send_goal_async(goal)\r\n        rclpy.spin_until_future_complete(self, future)\r\n        \r\n        goal_handle = future.result()\r\n        return goal_handle.accepted\r\n    \r\n    def pick_up(self, params):\r\n        \"\"\"Pick up object.\"\"\"\r\n        object_name = params.get('object', 'unknown')\r\n        self.get_logger().info(f'Picking up {object_name}')\r\n        # Implementation depends on robot\r\n        return True\r\n    \r\n    def place(self, params):\r\n        \"\"\"Place object.\"\"\"\r\n        object_name = params.get('object', 'unknown')\r\n        location = params.get('location', 'unknown')\r\n        self.get_logger().info(f'Placing {object_name} at {location}')\r\n        # Implementation depends on robot\r\n        return True\r\n    \r\n    def stop(self):\r\n        \"\"\"Stop robot.\"\"\"\r\n        cmd = Twist()\r\n        self.cmd_vel_pub.publish(cmd)\r\n        return True\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ActionExecutor()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-6-butler-controller-main-node",children:"Step 6: Butler Controller (Main Node)"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/nodes/butler_controller.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\"\"\"\r\nButler Controller\r\n\r\nMain controller coordinating all components.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom enum import Enum\r\n\r\nclass ButlerState(Enum):\r\n    IDLE = 'idle'\r\n    LISTENING = 'listening'\r\n    PLANNING = 'planning'\r\n    EXECUTING = 'executing'\r\n    COMPLETED = 'completed'\r\n    ERROR = 'error'\r\n\r\nclass ButlerController(Node):\r\n    def __init__(self):\r\n        super().__init__('butler_controller')\r\n        \r\n        # State\r\n        self.state = ButlerState.IDLE\r\n        \r\n        # Subscribers\r\n        self.status_sub = self.create_subscription(\r\n            String, '/butler/status', self.status_callback, 10\r\n        )\r\n        \r\n        # Publisher\r\n        self.state_pub = self.create_publisher(String, '/butler/state', 10)\r\n        \r\n        # Timer\r\n        self.timer = self.create_timer(0.1, self.controller_loop)\r\n        \r\n        self.get_logger().info('Butler controller started')\r\n        self.transition_to(ButlerState.LISTENING)\r\n    \r\n    def status_callback(self, msg):\r\n        \"\"\"Handle status updates.\"\"\"\r\n        status = msg.data\r\n        \r\n        if status == 'completed':\r\n            self.transition_to(ButlerState.COMPLETED)\r\n            # Return to listening after delay\r\n            self.create_timer(2.0, lambda: self.transition_to(ButlerState.LISTENING))\r\n    \r\n    def transition_to(self, new_state):\r\n        \"\"\"Transition to new state.\"\"\"\r\n        old_state = self.state\r\n        self.state = new_state\r\n        \r\n        self.get_logger().info(f'State: {old_state.value} -> {new_state.value}')\r\n        \r\n        # Publish state\r\n        state_msg = String()\r\n        state_msg.data = new_state.value\r\n        self.state_pub.publish(state_msg)\r\n    \r\n    def controller_loop(self):\r\n        \"\"\"Main control loop.\"\"\"\r\n        # Handle state-specific logic\r\n        if self.state == ButlerState.ERROR:\r\n            self.recover_from_error()\r\n    \r\n    def recover_from_error(self):\r\n        \"\"\"Recover from error.\"\"\"\r\n        self.get_logger().warn('Recovering from error')\r\n        self.transition_to(ButlerState.IDLE)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ButlerController()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Shutting down...')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-7-launch-file",children:"Step 7: Launch File"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"autonomous_butler/launch/butler.launch.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Voice recognizer\r\n        Node(\r\n            package='autonomous_butler',\r\n            executable='voice_recognizer',\r\n            name='voice_recognizer',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Vision processor\r\n        Node(\r\n            package='autonomous_butler',\r\n            executable='vision_processor',\r\n            name='vision_processor',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Task planner\r\n        Node(\r\n            package='autonomous_butler',\r\n            executable='task_planner',\r\n            name='task_planner',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Action executor\r\n        Node(\r\n            package='autonomous_butler',\r\n            executable='action_executor',\r\n            name='action_executor',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Butler controller\r\n        Node(\r\n            package='autonomous_butler',\r\n            executable='butler_controller',\r\n            name='butler_controller',\r\n            output='screen'\r\n        )\r\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-8-update-setuppy",children:"Step 8: Update setup.py"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\r\n\r\nsetup(\r\n    name='autonomous_butler',\r\n    version='0.0.1',\r\n    packages=['autonomous_butler'],\r\n    install_requires=['setuptools'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'voice_recognizer = autonomous_butler.nodes.voice_recognizer:main',\r\n            'vision_processor = autonomous_butler.nodes.vision_processor:main',\r\n            'task_planner = autonomous_butler.nodes.task_planner:main',\r\n            'action_executor = autonomous_butler.nodes.action_executor:main',\r\n            'butler_controller = autonomous_butler.nodes.butler_controller:main',\r\n        ],\r\n    },\r\n)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-and-debugging",children:"Testing and Debugging"}),"\n",(0,t.jsx)(n.h3,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,t.jsx)(n.h4,{id:"scenario-1-simple-navigation",children:"Scenario 1: Simple Navigation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Test command: "Go to the kitchen"\r\n# Expected: Robot navigates to kitchen location\n'})}),"\n",(0,t.jsx)(n.h4,{id:"scenario-2-object-manipulation",children:"Scenario 2: Object Manipulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Test command: "Pick up the cup"\r\n# Expected: Robot finds cup, navigates to it, picks it up\n'})}),"\n",(0,t.jsx)(n.h4,{id:"scenario-3-complex-task",children:"Scenario 3: Complex Task"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Test command: "Bring me a drink from the kitchen"\r\n# Expected: \r\n# 1. Navigate to kitchen\r\n# 2. Find drink\r\n# 3. Pick up drink\r\n# 4. Navigate to user\r\n# 5. Place drink\n'})}),"\n",(0,t.jsx)(n.h3,{id:"debugging-checklist",children:"Debugging Checklist"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Voice recognition working"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Vision detecting objects"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","LLM generating plans"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Actions executing correctly"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","State machine transitions working"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Error handling functional"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All nodes communicating"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"issue-1-voice-not-recognized",children:"Issue 1: Voice not recognized"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check microphone, reduce noise, speak clearly"]}),"\n",(0,t.jsx)(n.h4,{id:"issue-2-objects-not-detected",children:"Issue 2: Objects not detected"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Improve lighting, check camera, verify YOLO model"]}),"\n",(0,t.jsx)(n.h4,{id:"issue-3-plans-not-generated",children:"Issue 3: Plans not generated"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check OpenAI API key, verify internet connection"]}),"\n",(0,t.jsx)(n.h4,{id:"issue-4-actions-not-executing",children:"Issue 4: Actions not executing"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),": Check action executor, verify robot hardware"]}),"\n",(0,t.jsx)(n.h2,{id:"demo-scenarios",children:"Demo Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"demo-1-basic-commands",children:"Demo 1: Basic Commands"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Move forward"'})," - Robot moves forward"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Turn left"'})," - Robot turns left"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Stop"'})," - Robot stops"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"demo-2-object-interaction",children:"Demo 2: Object Interaction"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Find the cup"'})," - Robot searches and finds cup"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Pick up the cup"'})," - Robot picks up cup"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Bring it here"'})," - Robot brings cup to user"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"demo-3-complex-task",children:"Demo 3: Complex Task"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Go to the kitchen and check if the door is open"'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot navigates to kitchen"}),"\n",(0,t.jsx)(n.li,{children:"Checks door status"}),"\n",(0,t.jsx)(n.li,{children:"Reports back"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"demo-4-multi-step-task",children:"Demo 4: Multi-Step Task"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Clean up the room"'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot identifies objects"}),"\n",(0,t.jsx)(n.li,{children:"Plans cleaning sequence"}),"\n",(0,t.jsx)(n.li,{children:"Executes cleaning actions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,t.jsx)(n.h3,{id:"functionality-40-points",children:"Functionality (40 points)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Voice Recognition"})," (10 points)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Recognizes commands accurately"}),"\n",(0,t.jsx)(n.li,{children:"Handles errors gracefully"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Vision Processing"})," (10 points)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Detects objects correctly"}),"\n",(0,t.jsx)(n.li,{children:"Provides scene descriptions"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task Planning"})," (10 points)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Generates valid plans"}),"\n",(0,t.jsx)(n.li,{children:"Handles complex commands"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Action Execution"})," (10 points)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Executes actions correctly"}),"\n",(0,t.jsx)(n.li,{children:"Completes tasks successfully"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"code-quality-20-points",children:"Code Quality (20 points)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Structure"})," (5 points): Well-organized code"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation"})," (5 points): Clear comments and docstrings"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"})," (5 points): Robust error handling"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Best Practices"})," (5 points): Follows ROS 2 conventions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration-20-points",children:"Integration (20 points)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Component Integration"})," (10 points): All components work together"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Communication"})," (10 points): Proper topic/service usage"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-10-points",children:"Performance (10 points)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"})," (5 points): Response time < 1 second"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reliability"})," (5 points): Handles edge cases"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"documentation-10-points",children:"Documentation (10 points)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"README"})," (5 points): Clear setup instructions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Demo Video"})," (5 points): Working demonstration"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,t.jsx)(n.h3,{id:"expected-performance",children:"Expected Performance"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Target"}),(0,t.jsx)(n.th,{children:"Actual"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Voice Recognition Latency"}),(0,t.jsx)(n.td,{children:"< 500ms"}),(0,t.jsx)(n.td,{children:"~300ms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vision Processing"}),(0,t.jsx)(n.td,{children:"< 100ms"}),(0,t.jsx)(n.td,{children:"~80ms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Plan Generation"}),(0,t.jsx)(n.td,{children:"< 1000ms"}),(0,t.jsx)(n.td,{children:"~600ms"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Action Execution"}),(0,t.jsx)(n.td,{children:"Variable"}),(0,t.jsx)(n.td,{children:"Depends on task"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Total Response Time"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"< 2s"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"~1.5s"})})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"cost-analysis",children:"Cost Analysis"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Per Command:\r\n- Voice Recognition: $0 (Free)\r\n- Vision Processing: $0 (Local)\r\n- LLM Planning: $0.0003 (GPT-3.5-turbo)\r\n- Total: $0.0003 per command\r\n\r\nWith $5 credit: ~16,000 commands\n"})}),"\n",(0,t.jsx)(n.h2,{id:"deployment-tips",children:"Deployment Tips"}),"\n",(0,t.jsx)(n.h3,{id:"1-production-considerations",children:"1. Production Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use more robust error handling"}),"\n",(0,t.jsx)(n.li,{children:"Add logging and monitoring"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety checks"}),"\n",(0,t.jsx)(n.li,{children:"Add user feedback mechanisms"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-optimization",children:"2. Optimization"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Cache common plans"}),"\n",(0,t.jsx)(n.li,{children:"Use smaller models when possible"}),"\n",(0,t.jsx)(n.li,{children:"Optimize image processing"}),"\n",(0,t.jsx)(n.li,{children:"Reduce API calls"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-scaling",children:"3. Scaling"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Add more action primitives"}),"\n",(0,t.jsx)(n.li,{children:"Support multiple languages"}),"\n",(0,t.jsx)(n.li,{children:"Integrate with more sensors"}),"\n",(0,t.jsx)(n.li,{children:"Add learning capabilities"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-4-summary",children:"Module 4 Summary"}),"\n",(0,t.jsx)(n.p,{children:"Congratulations! You've completed Module 4: Vision-Language-Action. You now understand:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 VLA architecture and free tools"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Free voice recognition"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 LLM integration ($5 OpenAI)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Action planning and execution"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Multimodal systems"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Complete autonomous butler"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Total Cost: $5 (one-time, for learning)"})}),"\n",(0,t.jsx)(n.h2,{id:"course-summary",children:"Course Summary"}),"\n",(0,t.jsx)(n.p,{children:"You've completed all four modules:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Module 1: ROS 2 Fundamentals"})," - Robot operating system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Module 2: Simulation & Gazebo"})," - Virtual robot environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Module 3: AI-Powered Perception"})," - Free vision and SLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Module 4: Vision-Language-Action"})," - Natural language control"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"You can now build amazing robots for FREE!"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Apply Your Knowledge"}),": Build your own robot projects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Share Your Work"}),": Contribute to the community"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Keep Learning"}),": Explore advanced topics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build the Future"}),": Create the next generation of robots"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Keep building amazing robots! \ud83e\udd16\u2728"})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var s=r(6540);const t={},o=s.createContext(t);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);