"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[4846],{4611:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-04-vla/chapter-04-action-prediction","title":"Chapter 4: Action Prediction","description":"Introduction","source":"@site/docs/module-04-vla/chapter-04-action-prediction.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/chapter-04-action-prediction","permalink":"/physical-ai-textbook/docs/module-04-vla/chapter-04-action-prediction","draft":false,"unlisted":false,"editUrl":"https://github.com/Zeenat-Somroo911/physical-ai-textbook/edit/main/docs/module-04-vla/chapter-04-action-prediction.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5}}');var t=i(4848),s=i(8453);const a={sidebar_position:5},c="Chapter 4: Action Prediction",o={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Action Spaces",id:"action-spaces",level:2},{value:"Discrete Actions",id:"discrete-actions",level:3},{value:"Continuous Actions",id:"continuous-actions",level:3},{value:"Action Prediction Architectures",id:"action-prediction-architectures",level:2},{value:"Direct Prediction",id:"direct-prediction",level:3},{value:"Hierarchical Prediction",id:"hierarchical-prediction",level:3},{value:"Imitation Learning",id:"imitation-learning",level:2},{value:"Behavioral Cloning",id:"behavioral-cloning",level:3},{value:"Dataset Aggregation (DAgger)",id:"dataset-aggregation-dagger",level:3},{value:"Reinforcement Learning",id:"reinforcement-learning",level:2},{value:"Policy Gradient",id:"policy-gradient",level:3},{value:"Actor-Critic",id:"actor-critic",level:3},{value:"Action Chunking",id:"action-chunking",level:2},{value:"Predicting Action Sequences",id:"predicting-action-sequences",level:3},{value:"Safety and Constraints",id:"safety-and-constraints",level:2},{value:"Action Constraints",id:"action-constraints",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-4-action-prediction",children:"Chapter 4: Action Prediction"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Action prediction is the final stage of VLA systems, generating robot actions from visual observations and language instructions."}),"\n",(0,t.jsx)(n.h2,{id:"action-spaces",children:"Action Spaces"}),"\n",(0,t.jsx)(n.h3,{id:"discrete-actions",children:"Discrete Actions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class DiscreteActionDecoder(nn.Module):\r\n    def __init__(self, num_actions):\r\n        super().__init__()\r\n        self.action_head = nn.Linear(hidden_dim, num_actions)\r\n    \r\n    def forward(self, features):\r\n        logits = self.action_head(features)\r\n        action = torch.argmax(logits, dim=-1)\r\n        return action\n"})}),"\n",(0,t.jsx)(n.h3,{id:"continuous-actions",children:"Continuous Actions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ContinuousActionDecoder(nn.Module):\r\n    def __init__(self, action_dim):\r\n        super().__init__()\r\n        self.mean_head = nn.Linear(hidden_dim, action_dim)\r\n        self.std_head = nn.Linear(hidden_dim, action_dim)\r\n    \r\n    def forward(self, features):\r\n        mean = self.mean_head(features)\r\n        std = F.softplus(self.std_head(features)) + 1e-5\r\n        dist = torch.distributions.Normal(mean, std)\r\n        action = dist.sample()\r\n        return action, dist\n"})}),"\n",(0,t.jsx)(n.h2,{id:"action-prediction-architectures",children:"Action Prediction Architectures"}),"\n",(0,t.jsx)(n.h3,{id:"direct-prediction",children:"Direct Prediction"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class DirectActionPredictor(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = MultimodalEncoder()\r\n        self.action_decoder = ActionDecoder()\r\n    \r\n    def forward(self, image, instruction):\r\n        features = self.encoder(image, instruction)\r\n        action = self.action_decoder(features)\r\n        return action\n"})}),"\n",(0,t.jsx)(n.h3,{id:"hierarchical-prediction",children:"Hierarchical Prediction"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class HierarchicalActionPredictor(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = MultimodalEncoder()\r\n        self.high_level = HighLevelPlanner()\r\n        self.low_level = LowLevelController()\r\n    \r\n    def forward(self, image, instruction):\r\n        features = self.encoder(image, instruction)\r\n        \r\n        # High-level: "pick up cup"\r\n        high_level_action = self.high_level(features)\r\n        \r\n        # Low-level: joint angles, velocities\r\n        low_level_action = self.low_level(features, high_level_action)\r\n        return low_level_action\n'})}),"\n",(0,t.jsx)(n.h2,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,t.jsx)(n.h3,{id:"behavioral-cloning",children:"Behavioral Cloning"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def behavioral_cloning_loss(predicted_actions, expert_actions):\r\n    loss = F.mse_loss(predicted_actions, expert_actions)\r\n    return loss\r\n\r\n# Training\r\nfor image, instruction, expert_action in dataloader:\r\n    predicted = model(image, instruction)\r\n    loss = behavioral_cloning_loss(predicted, expert_action)\r\n    loss.backward()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"dataset-aggregation-dagger",children:"Dataset Aggregation (DAgger)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def dagger_training(model, expert_policy):\r\n    dataset = initial_dataset\r\n    \r\n    for iteration in range(num_iterations):\r\n        # Train on current dataset\r\n        train(model, dataset)\r\n        \r\n        # Collect new data with current policy\r\n        new_data = []\r\n        for state in environment:\r\n            action = model(state)\r\n            expert_action = expert_policy(state)\r\n            new_data.append((state, expert_action))\r\n        \r\n        # Aggregate datasets\r\n        dataset = dataset + new_data\n"})}),"\n",(0,t.jsx)(n.h2,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,t.jsx)(n.h3,{id:"policy-gradient",children:"Policy Gradient"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def policy_gradient_loss(actions, rewards, log_probs):\r\n    # REINFORCE\r\n    returns = compute_returns(rewards)\r\n    loss = -torch.mean(log_probs * returns)\r\n    return loss\n"})}),"\n",(0,t.jsx)(n.h3,{id:"actor-critic",children:"Actor-Critic"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ActorCritic(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.actor = ActionDecoder()\r\n        self.critic = ValueEstimator()\r\n    \r\n    def forward(self, image, instruction):\r\n        features = self.encoder(image, instruction)\r\n        action_dist = self.actor(features)\r\n        value = self.critic(features)\r\n        return action_dist, value\n"})}),"\n",(0,t.jsx)(n.h2,{id:"action-chunking",children:"Action Chunking"}),"\n",(0,t.jsx)(n.h3,{id:"predicting-action-sequences",children:"Predicting Action Sequences"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ActionChunking(nn.Module):\r\n    def __init__(self, chunk_size=10):\r\n        super().__init__()\r\n        self.encoder = MultimodalEncoder()\r\n        self.decoder = nn.LSTM(hidden_dim, action_dim, num_layers=2)\r\n    \r\n    def forward(self, image, instruction):\r\n        features = self.encoder(image, instruction)\r\n        \r\n        # Predict sequence of actions\r\n        hidden = features.unsqueeze(0)\r\n        actions = []\r\n        for _ in range(chunk_size):\r\n            action, hidden = self.decoder(hidden)\r\n            actions.append(action)\r\n        \r\n        return torch.stack(actions)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"safety-and-constraints",children:"Safety and Constraints"}),"\n",(0,t.jsx)(n.h3,{id:"action-constraints",children:"Action Constraints"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ConstrainedActionDecoder(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.action_decoder = ActionDecoder()\r\n    \r\n    def forward(self, features, constraints):\r\n        raw_action = self.action_decoder(features)\r\n        \r\n        # Apply constraints\r\n        constrained_action = apply_constraints(\r\n            raw_action,\r\n            max_velocity=constraints['max_vel'],\r\n            joint_limits=constraints['joint_limits']\r\n        )\r\n        return constrained_action\n"})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Success Rate"}),": Task completion percentage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Path Length"}),": Efficiency of actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Smoothness"}),": Action trajectory smoothness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Constraint violations"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use appropriate action representation"}),"\n",(0,t.jsx)(n.li,{children:"Consider temporal dependencies"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety constraints"}),"\n",(0,t.jsx)(n.li,{children:"Balance exploration and exploitation"}),"\n",(0,t.jsx)(n.li,{children:"Test in simulation before real robot"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement action decoder"}),"\n",(0,t.jsx)(n.li,{children:"Train with imitation learning"}),"\n",(0,t.jsx)(n.li,{children:"Add action constraints"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Chapter 5 covers building end-to-end VLA systems."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>c});var r=i(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);